{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "import utils.for_empatica as empatica\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pluma-python API  \n",
    "from modules import *\n",
    "from pluma.schema.outdoor import build_schema\n",
    "from missing_sync import build_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export empatica to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "import utils.for_empatica as empatica\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                csv_outdir = os.path.join(path.sourcedata, 'supp', 'stress_csv', f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                if not os.path.exists(csv_outdir): \n",
    "                    os.makedirs(csv_outdir)\n",
    "                try: # Try to load the dataset\n",
    "                    datapicker = create_datapicker(path = session_path, schema=build_schema)\n",
    "                    dataset    = load_dataset(datapicker.selected_path,schema=build_schema)\n",
    "                except:\n",
    "                    try: # Try to load by other schema\n",
    "                        print(\"Trying to load by other schema\")\n",
    "                        datapicker = create_datapicker(path=session_path, schema=build_schema, calibrate_ubx_to_harp=False)\n",
    "                        dataset    = load_dataset(datapicker.selected_path, ubx=True, unity=False, calibrate_ubx_to_harp=False, schema=build_schema)\n",
    "                        empatica.empatica_and_ecg_to_csv(dataset, csv_outdir)\n",
    "                    except:\n",
    "                        print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "                # Save the data to csv\n",
    "                empatica.empatica_and_ecg_to_csv(dataset, csv_outdir) \n",
    "                print(f'Successfully saved data for {participant_folder}-{session_name}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export metrics as 1Hz data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                input_directory = os.path.join(path.sourcedata, 'supp', 'stress_csv',f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                output_directory = os.path.join(input_directory, '_1hz')\n",
    "                if not os.path.exists(output_directory):\n",
    "                    os.makedirs(output_directory)\n",
    "                # Resample Empatica data\n",
    "                try:\n",
    "                    empatica.export_resampled_empatica_data(input_directory, output_directory)\n",
    "                except:\n",
    "                    print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations (Physiological and UTCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Define paths and session information\n",
    "session_name = 'Lapa' # Replace with your session name\n",
    "subject_folder = 'sub-OE009' # Replace with your subject folder name\n",
    "# Update these variables according to your directory structure\n",
    "input_directory = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, 'ses-'+session_name) \n",
    "output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, 'ses-'+session_name)       \n",
    "empatica_data_path = os.path.join(input_directory, '_1hz', 'data_all_1Hz.csv')\n",
    "sourcedata_path = os.path.join(path.sourcedata, 'supp') # Replace with the path to 'sourcedata' directory\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load Empatica data\n",
    "print(\"Loading Empatica data...\")\n",
    "empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "\n",
    "# Ensure 'DateTime' is in datetime format\n",
    "if empatica_data['DateTime'].dtype != 'datetime64[ns]':\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "\n",
    "# Remove any rows with NaT in 'DateTime'\n",
    "empatica_data = empatica_data.dropna(subset=['DateTime'])\n",
    "\n",
    "# Floor to seconds to ensure alignment\n",
    "empatica_data['DateTime'] = empatica_data['DateTime'].dt.floor('S')\n",
    "\n",
    "# --- Climate Data Processing ---\n",
    "\n",
    "# Locate the climate data CSV file\n",
    "print(\"Locating climate data CSV file...\")\n",
    "log2_path = os.path.join(sourcedata_path, 'log2')\n",
    "subject_path = os.path.join(log2_path, subject_folder)\n",
    "\n",
    "# Find session folder containing the session name\n",
    "session_folder = None\n",
    "for folder in os.listdir(subject_path):\n",
    "    if session_name in folder:\n",
    "        session_folder = folder\n",
    "        break\n",
    "\n",
    "if session_folder is None:\n",
    "    print(f\"Session folder containing '{session_name}' not found in {subject_path}.\")\n",
    "else:\n",
    "    session_path = os.path.join(subject_path, session_folder)\n",
    "\n",
    "    # Find the CSV file containing 'geodata_processed' in the filename\n",
    "    climate_csv_file = None\n",
    "    for file in os.listdir(session_path):\n",
    "        if 'geodata_processed' in file and file.endswith('.csv'):\n",
    "            climate_csv_file = file\n",
    "            break\n",
    "\n",
    "    if climate_csv_file is None:\n",
    "        print(f\"No CSV file containing 'geodata_processed' found in {session_path}.\")\n",
    "    else:\n",
    "        climate_csv_path = os.path.join(session_path, climate_csv_file)\n",
    "        print(f\"Climate data CSV file found: {climate_csv_path}\")\n",
    "\n",
    "        # Read the climate data CSV file\n",
    "        print(\"Reading climate data...\")\n",
    "        climate_data = pd.read_csv(climate_csv_path)\n",
    "\n",
    "        # Ensure datetime column is parsed correctly\n",
    "        if 'DateTime' in climate_data.columns:\n",
    "            climate_data['DateTime'] = pd.to_datetime(climate_data['DateTime'], errors='coerce')\n",
    "        else:\n",
    "            print(\"No 'DateTime' column found in climate data. Please ensure the CSV contains datetime information.\")\n",
    "            climate_data = None\n",
    "\n",
    "        # Remove any rows with NaT in 'DateTime'\n",
    "        if climate_data is not None:\n",
    "            climate_data = climate_data.dropna(subset=['DateTime'])\n",
    "\n",
    "        # Check if 'utci' column exists\n",
    "        if climate_data is not None and 'utci' not in climate_data.columns:\n",
    "            print(\"'utci' column not found in climate data.\")\n",
    "            climate_data = None\n",
    "\n",
    "        if climate_data is not None:\n",
    "            # Check if 'second' column exists\n",
    "            if 'second' in climate_data.columns:\n",
    "                # Combine 'DateTime' and 'second' to create a full datetime with seconds\n",
    "                print(\"Combining 'DateTime' and 'second' to create full datetime with seconds...\")\n",
    "                # Floor 'DateTime' to the nearest minute\n",
    "                climate_data['DateTime'] = climate_data['DateTime'].dt.floor('min')\n",
    "                # Add 'second' column as timedelta\n",
    "                climate_data['DateTime'] += pd.to_timedelta(climate_data['second'], unit='s')\n",
    "                # Now 'DateTime' includes seconds\n",
    "            else:\n",
    "                print(\"'second' column not found in climate data.\")\n",
    "                climate_data = None\n",
    "\n",
    "            # Ensure 'DateTime' is in datetime format\n",
    "            if climate_data is not None and climate_data['DateTime'].dtype != 'datetime64[ns]':\n",
    "                climate_data['DateTime'] = pd.to_datetime(climate_data['DateTime'], errors='coerce')\n",
    "                climate_data = climate_data.dropna(subset=['DateTime'])\n",
    "\n",
    "            # Check data type\n",
    "            print(\"Climate data 'DateTime' dtype:\", climate_data['DateTime'].dtype)\n",
    "\n",
    "            # Proceed if 'DateTime' and 'utci' are available\n",
    "            if climate_data is not None:\n",
    "                # Merge Empatica data with climate data on 'DateTime'\n",
    "                print(\"Merging Empatica data with climate data...\")\n",
    "                combined_data = pd.merge(empatica_data, climate_data[['DateTime', 'utci']], on='DateTime', how='inner')\n",
    "\n",
    "                # Save the combined data to CSV with proper datetime formatting\n",
    "                combined_data.to_csv(os.path.join(output_dir, 'combined_data.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Perform correlations between 'utci' and Empatica variables\n",
    "                print(\"Performing correlations between 'utci' and Empatica variables...\")\n",
    "                empatica_vars = combined_data.select_dtypes(include=[np.number]).columns.drop('utci')\n",
    "                correlation_results = []\n",
    "\n",
    "                for var in empatica_vars:\n",
    "                    # Drop NaN values for the pair\n",
    "                    valid_data = combined_data[['utci', var]].dropna()\n",
    "                    if len(valid_data) < 2:\n",
    "                        print(f\"Not enough data to compute correlation between 'utci' and '{var}'.\")\n",
    "                        continue\n",
    "\n",
    "                    # Pearson correlation\n",
    "                    pearson_corr, pearson_p = pearsonr(valid_data['utci'], valid_data[var])\n",
    "\n",
    "                    # Spearman correlation\n",
    "                    spearman_corr, spearman_p = spearmanr(valid_data['utci'], valid_data[var])\n",
    "\n",
    "                    # Append results\n",
    "                    correlation_results.append({\n",
    "                        'Variable': var,\n",
    "                        'Pearson_Correlation': pearson_corr,\n",
    "                        'Pearson_p_value': pearson_p,\n",
    "                        'Spearman_Correlation': spearman_corr,\n",
    "                        'Spearman_p_value': spearman_p\n",
    "                    })\n",
    "\n",
    "                    # Plot scatter plot with best-fit line\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "\n",
    "                    # Scatter plot\n",
    "                    plt.scatter(valid_data['utci'], valid_data[var], color='blue', alpha=0.6, edgecolor='k', label='Data')\n",
    "\n",
    "                    # Best-fit line using linear regression (OLS)\n",
    "                    slope, intercept, r_value, p_value, std_err = linregress(valid_data['utci'], valid_data[var])\n",
    "                    x_vals = np.array([valid_data['utci'].min(), valid_data['utci'].max()])\n",
    "                    y_vals = intercept + slope * x_vals\n",
    "                    plt.plot(x_vals, y_vals, color='red', linewidth=2, label='OLS Linear Regression')\n",
    "\n",
    "                    # Aesthetics adjustments\n",
    "                    plt.xlabel('Universal Thermal Climate Index (UTCI)', fontsize=14)\n",
    "                    plt.ylabel(var, fontsize=14)\n",
    "                    plt.title(f'Relationship between {var} and UTCI', fontsize=16)\n",
    "                    plt.legend(frameon=False, fontsize=12)\n",
    "\n",
    "                    # Remove background grid and frame\n",
    "                    plt.gca().spines['top'].set_visible(False)\n",
    "                    plt.gca().spines['right'].set_visible(False)\n",
    "                    plt.gca().spines['left'].set_linewidth(1.5)\n",
    "                    plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "                    # Include Pearson and Spearman correlation coefficients and p-values in the plot\n",
    "                    textstr = '\\n'.join((\n",
    "                        r'Pearson $r=%.2f$ (p=%.2e)' % (pearson_corr, pearson_p),\n",
    "                        r'Spearman $\\rho=%.2f$ (p=%.2e)' % (spearman_corr, spearman_p)\n",
    "                    ))\n",
    "                    # Place text box in upper left in axes coords\n",
    "                    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "                    # Tight layout for better spacing\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    # Save the figure\n",
    "                    plt.savefig(os.path.join(output_dir, f'scatter_{var}_vs_utci.png'), dpi=300)\n",
    "                    plt.close()\n",
    "\n",
    "                # Save correlation results to CSV\n",
    "                print(\"Saving correlation results...\")\n",
    "                correlation_df = pd.DataFrame(correlation_results)\n",
    "                correlation_df.to_csv(os.path.join(output_dir, 'correlation_results.csv'), index=False)\n",
    "\n",
    "                print(\"Correlation analysis complete. Results saved to the output directory.\")\n",
    "            else:\n",
    "                print(\"Climate data could not be processed due to missing 'DateTime', 'utci', or 'second' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matrices\n",
    "Perform correlations for:\n",
    "- within subjects\n",
    "- within paths\n",
    "- between paths\n",
    "- between subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_covariance_matrix(data, method='pairwise'):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix using different methods to handle missing data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with numeric variables.\n",
    "        method (str): Method to handle missing values. Options are:\n",
    "            - 'pairwise': Computes pairwise covariances, ignoring missing values.\n",
    "            - 'mean_impute': Fills missing values with the column mean.\n",
    "            - 'median_impute': Fills missing values with the column median.\n",
    "            - 'interpolate': Uses linear interpolation to fill missing values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Covariance matrix.\n",
    "    \"\"\"\n",
    "    if method == 'pairwise':\n",
    "        # Pandas computes pairwise covariances by default\n",
    "        return data.cov()\n",
    "    elif method == 'mean_impute':\n",
    "        filled_data = data.fillna(data.mean())\n",
    "    elif method == 'median_impute':\n",
    "        filled_data = data.fillna(data.median())\n",
    "    elif method == 'interpolate':\n",
    "        filled_data = data.interpolate(method='linear', limit_direction='both')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return filled_data.cov()\n",
    "\n",
    "def process_data(empatica_data_path, climate_data_path, output_dir):\n",
    "    \n",
    "    # Load Empatica data\n",
    "    empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "    empatica_data = empatica_data.dropna(subset=['DateTime']).set_index('DateTime')\n",
    "\n",
    "    # Load Climate data\n",
    "    climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "    climate_data['time'] = pd.to_datetime(climate_data['time'], errors='coerce')\n",
    "    climate_data = climate_data.dropna(subset=['time']).set_index('time')\n",
    "\n",
    "    # Merge data\n",
    "    combined_data = pd.merge(empatica_data, climate_data[['utci']], left_index=True, right_index=True, how='inner')\n",
    "    combined_data.to_csv(os.path.join(output_dir, 'combined_data.csv'))\n",
    "\n",
    "    # Select numeric columns for covariance analysis\n",
    "    numeric_data = combined_data.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Compute covariance matrices with different methods\n",
    "    covariance_methods = ['pairwise', 'mean_impute', 'median_impute', 'interpolate']\n",
    "    for method in covariance_methods:\n",
    "        print(f\"Computing covariance matrix using {method} method...\")\n",
    "        covariance_matrix = compute_covariance_matrix(numeric_data, method=method)\n",
    "        covariance_matrix.to_csv(os.path.join(output_dir, f'covariance_matrix_{method}.csv'))\n",
    "        print(f\"Covariance matrix ({method}) saved.\")\n",
    "\n",
    "    print(\"Covariance matrix computations complete.\")\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    session_name = 'Lapa'\n",
    "    subject_folder = 'sub-OE009'\n",
    "    output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, f'ses-{session_name}')\n",
    "    empatica_data_path = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "    climate_data_path = os.path.join(path.sourcedata, 'supp', 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process data and compute covariance matrices\n",
    "    process_data(empatica_data_path, climate_data_path, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, ttest_rel, pearsonr, spearmanr\n",
    "\n",
    "###############################################################################\n",
    "# Choose which columns from geodata to include (besides \"time\") \n",
    "# Typically, you want numeric columns that might be correlated with physiological data.\n",
    "# Exclude obviously irrelevant columns like 'geometry' or duplicates.\n",
    "###############################################################################\n",
    "SELECTED_GEODATA_COLS = [\n",
    "    # Potential environment/climate columns\n",
    "    \"tk_airquality_iaqindex_value\",\"tk_airquality_temperature_value\",\"tk_airquality_humidity_value\",\n",
    "    \"tk_airquality_airpressure_value\",\"tk_soundpressurelevel_spl_value\",\"tk_humidity_humidity_value\",\n",
    "    \"tk_analogin_voltage_value\",\"tk_particulatematter_pm1_0_value\",\"tk_particulatematter_pm2_5_value\",\n",
    "    \"tk_particulatematter_pm10_0_value\",\"tk_dual0_20ma_solarlight_value\",\"tk_thermocouple_temperature_value\",\n",
    "    \"tk_ptc_airtemp_value\",\"atmos_northwind_value\",\"atmos_eastwind_value\",\"atmos_gustwind_value\",\n",
    "    \"atmos_airtemperature_value\",\"accelerometer_orientation_x\",\"accelerometer_orientation_y\",\"accelerometer_orientation_z\",\n",
    "    \"accelerometer_gyroscope_x\",\"accelerometer_gyroscope_y\",\"accelerometer_gyroscope_z\",\"accelerometer_linearaccl_x\",\n",
    "    \"accelerometer_linearaccl_y\",\"accelerometer_linearaccl_z\",\"accelerometer_magnetometer_x\",\"accelerometer_magnetometer_y\",\n",
    "    \"accelerometer_magnetometer_z\",\"accelerometer_accl_x\",\"accelerometer_accl_y\",\"accelerometer_accl_z\",\n",
    "    \"accelerometer_gravity_x\",\"accelerometer_gravity_y\",\"accelerometer_gravity_z\",\n",
    "    # Merged columns from subsequent steps\n",
    "    \"empatica_e4_gsr\",\"empatica_e4_hr\",\"empatica_e4_ibi\",\"empatica_e4_temperature\",\n",
    "    \"humidity\",\"wind_speed\",\"temp_atmos\",\"temp_tk\",\"temp_tk_ptc\",\"temp_radiant\",\"utci\",\n",
    "    \"day_of_year\",\"ghi\",\"hPa\",\"dew_point\",\"solar_altitude\",\"solar_azimuth\",\"mrt\"\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "def compute_covariance_matrix(data, method='pairwise'):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix using different methods to handle missing data.\n",
    "    \"\"\"\n",
    "    if method == 'pairwise':\n",
    "        # Pandas defaults to pairwise complete for .cov()\n",
    "        return data.cov()\n",
    "    elif method == 'mean_impute':\n",
    "        filled_data = data.fillna(data.mean(numeric_only=True))\n",
    "    elif method == 'median_impute':\n",
    "        filled_data = data.fillna(data.median(numeric_only=True))\n",
    "    elif method == 'interpolate':\n",
    "        filled_data = data.interpolate(method='linear', limit_direction='both')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return filled_data.cov()\n",
    "\n",
    "def compute_correlation_matrix(data, method='pearson'):\n",
    "    \"\"\"\n",
    "    Compute correlation matrix: 'pearson', 'spearman', or 'kendall'.\n",
    "    \"\"\"\n",
    "    return data.corr(method=method)\n",
    "\n",
    "def plot_heatmap(matrix, title, output_path):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the given matrix (DataFrame).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(matrix, annot=False, cmap='coolwarm', square=True)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def process_data(empatica_data_path, climate_data_path, output_dir):\n",
    "    \n",
    "    ###############################################################################\n",
    "    # Load Empatica Data\n",
    "    ###############################################################################\n",
    "    empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "    empatica_data.dropna(subset=['DateTime'], inplace=True)\n",
    "    empatica_data.set_index('DateTime', inplace=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Load \"geodata\" / climate data\n",
    "    ###############################################################################\n",
    "    climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "    climate_data['time'] = pd.to_datetime(climate_data['time'], errors='coerce')\n",
    "    climate_data.dropna(subset=['time'], inplace=True)\n",
    "    climate_data.set_index('time', inplace=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Merge\n",
    "    # We'll gather columns from climate_data that we want \n",
    "    # (some may not exist, so we will safely drop missing)\n",
    "    ###############################################################################\n",
    "    available_cols = [c for c in SELECTED_GEODATA_COLS if c in climate_data.columns]\n",
    "    # If none found, we proceed with a minimal set\n",
    "    if not available_cols:\n",
    "        print(\"No selected columns found in climate_data. Falling back to just utci if present.\")\n",
    "        if \"utci\" in climate_data.columns:\n",
    "            available_cols = [\"utci\"]\n",
    "        else:\n",
    "            print(\"No utci in climate data; the merge will produce minimal columns.\")\n",
    "            available_cols = climate_data.columns  # fallback to all\n",
    "\n",
    "    # Subset climate_data\n",
    "    climate_subset = climate_data[available_cols]\n",
    "\n",
    "    # Merge\n",
    "    combined_data = pd.merge(empatica_data, climate_subset, \n",
    "                             left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    # Export the final table\n",
    "    combined_data.to_csv(os.path.join(output_dir, 'combined_data_full.csv'))\n",
    "    print(f\"Exported final combined data with {len(combined_data)} rows, {combined_data.shape[1]} columns.\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # Covariance / Correlation\n",
    "    ###############################################################################\n",
    "    # We'll select numeric columns only\n",
    "    numeric_data = combined_data.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Possibly we drop columns that have zero variance or near-constant\n",
    "    # or too many NaNs, but let's keep it simple\n",
    "\n",
    "    # 1) Covariance with different methods\n",
    "    covariance_methods = ['pairwise', 'mean_impute', 'median_impute', 'interpolate']\n",
    "    for method in covariance_methods:\n",
    "        cov_mat = compute_covariance_matrix(numeric_data, method=method)\n",
    "        cov_mat.to_csv(os.path.join(output_dir, f'cov_matrix_{method}.csv'))\n",
    "        plot_heatmap(cov_mat, f\"Covariance Matrix [{method}]\", \n",
    "                     os.path.join(output_dir, f'cov_matrix_{method}.png'))\n",
    "\n",
    "    # 2) Correlation (Pearson, Spearman)\n",
    "    for corr_method in ['pearson','spearman']:\n",
    "        corr_mat = compute_correlation_matrix(numeric_data, method=corr_method)\n",
    "        corr_mat.to_csv(os.path.join(output_dir, f'corr_matrix_{corr_method}.csv'))\n",
    "        plot_heatmap(corr_mat, f\"Correlation Matrix [{corr_method}]\", \n",
    "                     os.path.join(output_dir, f'corr_matrix_{corr_method}.png'))\n",
    "\n",
    "    print(\"Covariance and correlation matrices saved & plotted.\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # MAIN SCRIPT\n",
    "    datadir = os.path.join(path.sourcedata, 'data')\n",
    "    for participant_folder in os.listdir(datadir):\n",
    "        if participant_folder.startswith(\"OE\"):\n",
    "            participant_path = os.path.join(datadir, participant_folder)\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                session_path = os.path.join(participant_path, session_folder)\n",
    "                if os.path.isdir(session_path):\n",
    "                    session_name = extract_session_name(session_folder)\n",
    "                    subject_folder = f'sub-{participant_folder}'\n",
    "                    output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, f'ses-{session_name}')\n",
    "                    empatica_data_path = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "                    climate_data_path = os.path.join(path.sourcedata, 'supp', 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "                    if not os.path.exists(output_dir):    \n",
    "                        os.makedirs(output_dir, exist_ok=True)  \n",
    "                    try:\n",
    "                        print(f\"Processing {participant_folder}-{session_name}...\")\n",
    "                        process_data(empatica_data_path, climate_data_path, output_dir)\n",
    "                    except:\n",
    "                        print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_session_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 203\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone entire pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m session_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(participant_path, session_folder)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(session_path):\n\u001b[1;32m--> 149\u001b[0m     session_name \u001b[38;5;241m=\u001b[39m \u001b[43mextract_session_name\u001b[49m(session_folder)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Build the paths\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     subject_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparticipant_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_session_name' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from utils import *\n",
    "\n",
    "###############################################################################\n",
    "# Lists for separation of variables\n",
    "# You can adapt these to your actual column names\n",
    "###############################################################################\n",
    "PHYS_VARS = [\n",
    "    \"empatica_e4_gsr\",\"empatica_e4_hr\",\"empatica_e4_ibi\",\"empatica_e4_temperature\",\n",
    "    # add other physiol columns if needed\n",
    "]\n",
    "\n",
    "CLIM_VARS = [\n",
    "    \"tk_airquality_iaqindex_value\",\"tk_airquality_temperature_value\",\"tk_airquality_humidity_value\",\n",
    "    \"tk_airquality_airpressure_value\",\"tk_soundpressurelevel_spl_value\",\"tk_humidity_humidity_value\",\n",
    "    \"tk_analogin_voltage_value\",\"tk_particulatematter_pm1_0_value\",\"tk_particulatematter_pm2_5_value\",\n",
    "    \"tk_particulatematter_pm10_0_value\",\"tk_dual0_20ma_solarlight_value\",\"tk_thermocouple_temperature_value\",\n",
    "    \"tk_ptc_airtemp_value\",\"atmos_northwind_value\",\"atmos_eastwind_value\",\"atmos_gustwind_value\",\n",
    "    \"atmos_airtemperature_value\",\"humidity\",\"wind_speed\",\"temp_atmos\",\"temp_tk\",\"temp_tk_ptc\",\n",
    "    \"temp_radiant\",\"utci\",\"day_of_year\",\"ghi\",\"hPa\",\"dew_point\",\"solar_altitude\",\"solar_azimuth\",\n",
    "    \"mrt\"\n",
    "]\n",
    "\n",
    "def do_within_session_correlations(combined_data, session_name, output_dir):\n",
    "    \"\"\"\n",
    "    For a single session, compute correlation between all PHYS_VARS x CLIM_VARS,\n",
    "    store in a DataFrame, and plot a heatmap.\n",
    "    \"\"\"\n",
    "    # Keep only columns that exist\n",
    "    available_phys = [v for v in PHYS_VARS if v in combined_data.columns]\n",
    "    available_clim = [v for v in CLIM_VARS if v in combined_data.columns]\n",
    "\n",
    "    if not (available_phys and available_clim):\n",
    "        print(f\"No overlapping columns for session={session_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # We can do pairwise correlation row by row\n",
    "    # First drop rows with all NaN in those columns\n",
    "    sub_df = combined_data[available_phys + available_clim].dropna(how='all')\n",
    "\n",
    "    # We'll create a result matrix shaped (len(phys), len(clim)) with correlation\n",
    "    corr_values = np.zeros((len(available_phys), len(available_clim)))\n",
    "    p_values    = np.zeros((len(available_phys), len(available_clim)))\n",
    "\n",
    "    for i, pv in enumerate(available_phys):\n",
    "        for j, cv in enumerate(available_clim):\n",
    "            # Drop row if either is NaN\n",
    "            pair_df = sub_df[[pv, cv]].dropna()\n",
    "            if len(pair_df) > 2:\n",
    "                r, p = pearsonr(pair_df[pv], pair_df[cv])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            corr_values[i,j] = r\n",
    "            p_values[i,j]    = p\n",
    "\n",
    "    # Create a DataFrame\n",
    "    corr_df = pd.DataFrame(corr_values, index=available_phys, columns=available_clim)\n",
    "    pval_df = pd.DataFrame(p_values,  index=available_phys, columns=available_clim)\n",
    "\n",
    "    # Save CSV\n",
    "    corr_df.to_csv(os.path.join(output_dir, f\"{session_name}_phys_clim_correlations.csv\"))\n",
    "    pval_df.to_csv(os.path.join(output_dir, f\"{session_name}_phys_clim_pvalues.csv\"))\n",
    "\n",
    "    # Plot heatmap of correlation (r-values)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(f\"PHYS-CLIM Correlations (Pearson R)\\nSession: {session_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{session_name}_phys_clim_correlation_heatmap.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Session={session_name}: correlation done. Saved to {output_dir}\")\n",
    "\n",
    "def do_between_sessions_correlations(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Compute correlation for all sessions combined (pooled data).\n",
    "    We'll do the same approach as do_within_session_correlations,\n",
    "    but on the entire dataset from all sessions.\n",
    "    \"\"\"\n",
    "    available_phys = [v for v in PHYS_VARS if v in all_data.columns]\n",
    "    available_clim = [v for v in CLIM_VARS if v in all_data.columns]\n",
    "\n",
    "    if not (available_phys and available_clim):\n",
    "        print(\"No overlapping columns for the entire dataset. Skipping between-sessions correlation.\")\n",
    "        return\n",
    "\n",
    "    # Drop rows with no data in those columns\n",
    "    sub_df = all_data[available_phys + available_clim].dropna(how='all')\n",
    "\n",
    "    corr_values = np.zeros((len(available_phys), len(available_clim)))\n",
    "    p_values    = np.zeros((len(available_phys), len(available_clim)))\n",
    "\n",
    "    for i, pv in enumerate(available_phys):\n",
    "        for j, cv in enumerate(available_clim):\n",
    "            pair_df = sub_df[[pv, cv]].dropna()\n",
    "            if len(pair_df) > 2:\n",
    "                r, p = pearsonr(pair_df[pv], pair_df[cv])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            corr_values[i,j] = r\n",
    "            p_values[i,j]    = p\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_values, index=available_phys, columns=available_clim)\n",
    "    pval_df = pd.DataFrame(p_values,  index=available_phys, columns=available_clim)\n",
    "\n",
    "    corr_df.to_csv(os.path.join(output_dir, \"ALL_physiol_clim_correlations.csv\"))\n",
    "    pval_df.to_csv(os.path.join(output_dir, \"ALL_physiol_clim_pvalues.csv\"))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(\"PHYS-CLIM Correlations (Pearson R)\\nAll Sessions Combined\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"ALL_physiol_clim_correlation_heatmap.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Between-sessions correlation done. Saved to output folder.\")\n",
    "\n",
    "def main():\n",
    "    # We'll do something similar to your existing approach:\n",
    "    # We iterate over participants/sessions. For each session, we merge data, store in memory, do correlation, etc.\n",
    "\n",
    "    # Example session map if needed\n",
    "    sessions_map = {\n",
    "       'Baixa': 4,\n",
    "       'Belem': 1,\n",
    "       'Parque': 6,\n",
    "       'Gulbenkian': 3,\n",
    "       'Lapa': 2,\n",
    "       'Graca': 5\n",
    "    }\n",
    "\n",
    "    # We'll keep an all_data list to do the between-sessions approach\n",
    "    all_data_frames = []\n",
    "    search_dir = os.path.join(path.sourcedata, 'data')\n",
    "    datadir = os.path.join(path.sourcedata, 'supp')\n",
    "    for participant_folder in os.listdir(search_dir):\n",
    "        if participant_folder.startswith(\"OE\"):\n",
    "            participant_path = os.path.join(search_dir, participant_folder)\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                session_path = os.path.join(participant_path, session_folder)\n",
    "                if os.path.isdir(session_path):\n",
    "                    session_name = extract_session_name(session_folder)\n",
    "                    # Build the paths\n",
    "                    subject_folder = f'sub-{participant_folder}'\n",
    "                    output_dir = os.path.join(datadir, \"correlation\", subject_folder, f'ses-{session_name}')\n",
    "                    empatica_data_path = os.path.join(datadir, 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "                    climate_data_path  = os.path.join(datadir, 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                    # 1) Merge\n",
    "                    #   We'll do a simplified approach here or adapt from your existing \"process_data\" function\n",
    "                    try:\n",
    "                        # Load Empatica\n",
    "                        empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "                        empatica_data.dropna(subset=['DateTime'], inplace=True)\n",
    "                        empatica_data.set_index('DateTime', inplace=True)\n",
    "\n",
    "                        # Load climate\n",
    "                        climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "                        climate_data.dropna(subset=['time'], inplace=True)\n",
    "                        climate_data.set_index('time', inplace=True)\n",
    "\n",
    "                        # Merge\n",
    "                        combined_data = pd.merge(empatica_data, climate_data, left_index=True, right_index=True, how='inner')\n",
    "                        # Possibly keep only numeric columns\n",
    "                        # But let's keep all columns for referencing\n",
    "                        combined_data.to_csv(os.path.join(output_dir, 'merged.csv'))\n",
    "                        \n",
    "                        # 2) Within-Session correlation\n",
    "                        do_within_session_correlations(combined_data, session_name, output_dir)\n",
    "\n",
    "                        # Keep for all-data approach\n",
    "                        # We'll add a column 'Session' so we know from which session each row is\n",
    "                        combined_data[\"Session\"] = session_name\n",
    "                        all_data_frames.append(combined_data)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error merging or analyzing {participant_folder}-{session_name}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    # 3) Between-Session correlation\n",
    "    if all_data_frames:\n",
    "        big_df = pd.concat(all_data_frames, axis=0, ignore_index=False)\n",
    "        # We'll do one big correlation\n",
    "        out_all_dir = os.path.join(datadir, \"correlation\", \"ALL_SESSIONS\")\n",
    "        os.makedirs(out_all_dir, exist_ok=True)\n",
    "        do_between_sessions_correlations(big_df, out_all_dir)\n",
    "    else:\n",
    "        print(\"No data frames combined. Possibly no merges succeeded.\")\n",
    "\n",
    "    print(\"Done entire pipeline.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
