{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize output data\n",
    "Run these cells sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "import utils.for_empatica as empatica\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pluma-python API  \n",
    "from modules import *\n",
    "from pluma.schema.outdoor import build_schema\n",
    "from missing_sync import build_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export lsl, geodata, and empatica streams to CSV\n",
    "\n",
    "Geodata is the backbone of this process since it is already sampled at 1 Hz and without it, it's impossible to synchronize all the data streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "import utils.for_empatica as empatica\n",
    "import utils.for_climate as climate\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir    = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                # Geodata output\n",
    "                output = os.path.join(path.sourcedata, 'supp', 'geodata', 'log', f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                if not os.path.exists(output): \n",
    "                    os.makedirs(output)\n",
    "                # Empatica output\n",
    "                csv_outdir = os.path.join(path.sourcedata, 'supp', 'stress_csv', f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                if not os.path.exists(csv_outdir): \n",
    "                    os.makedirs(csv_outdir)\n",
    "                try: # Try to load the dataset\n",
    "                    datapicker = create_datapicker(path = session_path, schema=build_schema)\n",
    "                    dataset    = load_dataset(datapicker.selected_path,schema=build_schema)\n",
    "                except:\n",
    "                    try: # Try to load by other schema\n",
    "                        print(\"Trying to load by other schema\")\n",
    "                        datapicker = create_datapicker(path=session_path, schema=build_schema, calibrate_ubx_to_harp=False)\n",
    "                        dataset    = load_dataset(datapicker.selected_path, ubx=True, unity=False, calibrate_ubx_to_harp=False, schema=build_schema)\n",
    "                    except:\n",
    "                        print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "                # Save geodata to csv\n",
    "                climate.geodata_to_csv(dataset, participant_folder, session_name, output)\n",
    "                # Save empatica data to csv\n",
    "                empatica.empatica_and_ecg_to_csv(dataset, csv_outdir) \n",
    "                print(f'Successfully saved data for {participant_folder}-{session_name}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export processed empatica at 1 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                input_directory = os.path.join(path.sourcedata, 'supp', 'stress_csv',f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                output_directory = os.path.join(input_directory, '_1hz')\n",
    "                if not os.path.exists(output_directory):\n",
    "                    os.makedirs(output_directory)\n",
    "                # Resample Empatica data\n",
    "                try:\n",
    "                    empatica.export_resampled_empatica_data(input_directory, output_directory)\n",
    "                except:\n",
    "                    print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Eye-tracking metrics (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "import utils.for_eye_tracker as et\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                csv_outdir = os.path.join(path.sourcedata, 'supp', 'gaze', f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                if not os.path.exists(csv_outdir): \n",
    "                    os.makedirs(csv_outdir)\n",
    "                try: # Try to load the dataset\n",
    "                    datapicker = create_datapicker(path = session_path, schema=build_schema)\n",
    "                    dataset    = load_dataset(datapicker.selected_path,schema=build_schema)\n",
    "                except:\n",
    "                    try: # Try to load by other schema\n",
    "                        print(\"Trying to load by other schema\")\n",
    "                        datapicker = create_datapicker(path=session_path, schema=build_schema, calibrate_ubx_to_harp=False)\n",
    "                        dataset    = load_dataset(datapicker.selected_path, ubx=True, unity=False, calibrate_ubx_to_harp=False, schema=build_schema)\n",
    "                    except:\n",
    "                        print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "                try:\n",
    "                    # Save the data to csv\n",
    "                    et.export_gaze_to_csv(dataset, csv_outdir) \n",
    "                    print(f'Successfully saved data for {participant_folder}-{session_name}!')\n",
    "                except:\n",
    "                    print(f\"Error in exporting gaze data for {participant_folder}-{session_name}\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add EEG and other informations to the table data\n",
    "Band power metrics have been computed on preprocessed EEG data using MATLAB. Other data like participant_id, age, sex, and path name are also to be added.\n",
    "Implemented:\n",
    "- EEG\n",
    "- Geodata\n",
    "- Empatica\n",
    "\n",
    "To do:\n",
    "- Add eye-tracking\n",
    "- Add HR\n",
    "- Add behavioral and phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import utils.for_setpath as path\n",
    "\n",
    "def tidy_data(data, subject_id, session_id, bidsroot):\n",
    "    \"\"\"Tidy the tabular data\n",
    "    Add participant id\n",
    "    Add session id\n",
    "    Add age\n",
    "    Add sex\n",
    "    Add path typology\n",
    "    Make gps columns (lon,lat,alt) the 2nd, 3rd, and 4th columns\n",
    "    Rename columns to be more informative\n",
    "    \"\"\"\n",
    "    # Add metadata\n",
    "    data['participant_id'] = subject_id\n",
    "    data['session_id'] = session_id\n",
    "    participants = pd.read_csv(os.path.join(bidsroot, 'participants.tsv'), sep='\\t')\n",
    "    data['age'] = participants[participants['participant_id'] == f'sub-{subject_id}']['age'].values[0]\n",
    "    data['sex'] = participants[participants['participant_id'] == f'sub-{subject_id}']['sex'].values[0]\n",
    "\n",
    "    # Add behavioral ratings\n",
    "    subj_num            = int(subject_id[2:])\n",
    "    id                  = infer_participant_code('lisbon', subj_num, session_id, 'all')\n",
    "    sam                 = fetch_beh_ratings(os.path.join(path.sourcedata,'supp'), id)\n",
    "    if sam.size == 0:  # If no behavioral data, add empty columns\n",
    "        data['valence']     = np.nan\n",
    "        data['arousal']     = np.nan\n",
    "        data['naturalness'] = np.nan\n",
    "        data['crowdedness'] = np.nan\n",
    "    else:  # Otherwise, add behavioral ratings\n",
    "        sam                 = np.tile(sam, (len(data), 1))\n",
    "        data['valence']     = sam[:, 0]\n",
    "        data['arousal']     = sam[:, 1]\n",
    "        data['naturalness'] = sam[:, 2]\n",
    "        data['crowdedness'] = sam[:, 3]\n",
    "\n",
    "    # Rename columns\n",
    "    rename_dict = {\n",
    "        # Time\n",
    "        'DateTime': 'time',\n",
    "\n",
    "        # Climate\n",
    "        'tk_airquality_iaqindex_value': 'air_quality_index',\n",
    "        'tk_airquality_temperature_value': 'air_temperature',\n",
    "        'tk_airquality_humidity_value': 'air_humidity',\n",
    "        'tk_airquality_airpressure_value': 'air_pressure',\n",
    "        'tk_soundpressurelevel_spl_value': 'sound_pressure_level',\n",
    "        'tk_humidity_humidity_value': 'humidity_sensor',\n",
    "        'tk_analogin_voltage_value': 'analog_voltage',\n",
    "        'tk_particulatematter_pm1_0_value': 'pm1_0',\n",
    "        'tk_particulatematter_pm2_5_value': 'pm2_5',\n",
    "        'tk_particulatematter_pm10_0_value': 'pm10_0',\n",
    "        'tk_dual0_20ma_solarlight_value': 'solar_light',\n",
    "        'tk_thermocouple_temperature_value': 'thermocouple_temp',\n",
    "        'tk_ptc_airtemp_value': 'ptc_air_temp',\n",
    "        'atmos_northwind_value': 'north_wind',\n",
    "        'atmos_eastwind_value': 'east_wind',\n",
    "        'atmos_gustwind_value': 'gust_wind',\n",
    "        'atmos_airtemperature_value': 'atmospheric_temperature',\n",
    "        'temp_atmos': 'temp_atmospheric',\n",
    "        'temp_tk': 'temp_tk',\n",
    "        'temp_tk_ptc': 'temp_tk_ptc',\n",
    "        'temp_radiant': 'radiant_temp',\n",
    "        'hPa': 'pressure',\n",
    "        'dew_point': 'dew_point_temperature',\n",
    "\n",
    "        # Empatica\n",
    "        'E4_HR_IBI': 'empatica_heart_rate',\n",
    "        'TEMP': 'skin_temperature',\n",
    "        'EDA_RAW': 'eda_raw',\n",
    "        'EDA_PHASIC': 'eda_phasic',\n",
    "        'AccX': 'x_acceleration',\n",
    "        'AccY': 'y_acceleration',\n",
    "        'AccZ': 'z_acceleration',\n",
    "        'Magnitude': 'acceleration_magnitude',\n",
    "        'BVP_Values': 'bvp',\n",
    "\n",
    "        # Other\n",
    "        'stress_category': 'utci_stress_category'\n",
    "    }\n",
    "    data.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Dynamically reorder columns\n",
    "    column_order = [\n",
    "        'time',  # Time-related column\n",
    "        'longitude', 'latitude', 'elevation',  # GPS columns\n",
    "        'participant_id', 'session_id', 'age', 'sex'  # Metadata columns\n",
    "    ]\n",
    "    # Identify remaining columns dynamically\n",
    "    other_columns = [col for col in data.columns if col not in column_order]\n",
    "    reordered_columns = column_order + other_columns\n",
    "\n",
    "    # Apply the reordered column structure\n",
    "    data = data[reordered_columns]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "datadir = os.path.join(path.sourcedata, 'data')\n",
    "for participant_folder in os.listdir(datadir):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(datadir, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                # Define paths of streams\n",
    "                empatica_data_path = os.path.join(path.sourcedata, 'supp', 'stress_csv', f'sub-{participant_folder}', f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv') \n",
    "                geodata_data_path  = os.path.join(path.sourcedata, 'supp', 'geodata', 'log', f'sub-{participant_folder}', f'ses-{session_name}', f'sub-{participant_folder}_ses-{session_name}_geodata.xlsx')\n",
    "                eeg_data_path      = os.path.join(path.results, 'analysis-climate_physio_eeg_model_pipeline-outdoor_literature', f'sub-{participant_folder}', f'ses-{session_name}', 'data', 'a01_psd_compute.m', 'eeg_power.xlsx')\n",
    "                # Define output path\n",
    "                output_path        = os.path.join(path.results, 'fulldata', f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "                if not os.path.exists(output_path): \n",
    "                    os.makedirs(output_path)\n",
    "                # Attempt to merge the data (first empatica and geodata, and then eeg)\n",
    "                # If any file is missing skip to the next data stream\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                #                             GEODATA                           #\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                # Load Geodata\n",
    "                print(\"Loading Geodata...\")\n",
    "                try:\n",
    "                    geodata_data = pd.read_excel(geodata_data_path,parse_dates=['time'])\n",
    "                except:\n",
    "                    print(\"There is no Geodata for this session\")\n",
    "                    continue\n",
    "                geodata_data = geodata_data.rename(columns={'time':'DateTime'})\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                #                            EMPATICA                           #\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                # Load Empatica data\n",
    "                print(\"Loading Empatica data...\")\n",
    "                try:\n",
    "                    empatica_data = pd.read_csv(empatica_data_path,parse_dates=['DateTime'])\n",
    "                except:   \n",
    "                    print(\"There is no Empatica data for this session\")\n",
    "                    print(\"Adding empty Empatica data...\")\n",
    "                    # Create an empty DataFrame with the same column structure\n",
    "                    empatica_columns = [\n",
    "                        \"DateTime\", \"E4_HR\", \"E4_HR_IBI\", \"TEMP\", \"EDA_RAW\",\n",
    "                        \"EDA_PHASIC\", \"AccX\", \"AccY\", \"AccZ\", \"Magnitude\", \"BVP_Values\"\n",
    "                    ]\n",
    "                    empatica_data = pd.DataFrame(columns=empatica_columns)\n",
    "                    empatica_data['DateTime'] = geodata_data['DateTime']\n",
    "                # Merge Empatica data with climate data on 'DateTime'\n",
    "                print(\"Merging Empatica data with climate data...\")\n",
    "                combined_data = pd.merge(empatica_data, geodata_data, on='DateTime', how='inner')\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                #                            EEG                                #\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                # Load EEG data\n",
    "                print(\"Loading EEG data...\")\n",
    "                try:\n",
    "                    eeg_data = pd.read_excel(eeg_data_path)\n",
    "                except:\n",
    "                    print(\"There is no EEG data for this session\")\n",
    "                    print(\"Adding empty EEG data...\")\n",
    "                    # Create an empty DataFrame with the same column structure\n",
    "                    eeg_columns = [\n",
    "                        \"Time\", \"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\",\n",
    "                        \"frontal alpha\", \"frontal midline theta\", \"theta-beta ratio\",\t\n",
    "                        \"frontal alpha asymmetry\", \"frontal theta\"\n",
    "                    ]\n",
    "                    eeg_data = pd.DataFrame(columns=eeg_columns)\n",
    "                eeg_data = eeg_data.rename(columns={'Time':'DateTime'})\n",
    "                if eeg_data.empty:\n",
    "                    eeg_data['DateTime'] = combined_data['DateTime']\n",
    "                    # Merge EEG data with combined data on 'DateTime'\n",
    "                    print(\"Merging empty EEG data with combined data...\")\n",
    "                    combined_data = pd.merge(combined_data, eeg_data, on='DateTime', how='left')\n",
    "                else:\n",
    "                    # Merge EEG data with combined data on 'DateTime'\n",
    "                    print(\"Merging EEG data with combined data...\")\n",
    "                    combined_data = pd.merge(combined_data, eeg_data, on='DateTime', how='inner')\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                #                            TIDY                               #\n",
    "                # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "                # Tidy combined data\n",
    "                print(\"Tidying combined data...\")\n",
    "                combined_data = tidy_data(combined_data, participant_folder, session_name, path.bidsroot)\n",
    "                # Export combined data\n",
    "                print(\"Exporting combined data...\")\n",
    "                combined_data.to_csv(os.path.join(output_path, 'alldata.csv'), index=False)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlations (Physiological and UTCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Define paths and session information\n",
    "session_name = 'Lapa' # Replace with your session name\n",
    "subject_folder = 'sub-OE009' # Replace with your subject folder name\n",
    "# Update these variables according to your directory structure\n",
    "sourcedata_path = os.path.join(path.sourcedata, 'supp') # Replace with the path to 'sourcedata' directory\n",
    "input_directory = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, 'ses-'+session_name) \n",
    "empatica_data_path = os.path.join(input_directory, '_1hz', 'data_all_1Hz.csv')\n",
    "geodata_data_path = os.path.join(sourcedata_path, 'geodata', 'log', subject_folder, 'ses-'+session_name, f'sub-{participant_folder}', f'ses-{session_name}')\n",
    "output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, 'ses-'+session_name)       \n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load Empatica data\n",
    "print(\"Loading Empatica data...\")\n",
    "empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "\n",
    "# Ensure 'DateTime' is in datetime format\n",
    "if empatica_data['DateTime'].dtype != 'datetime64[ns]':\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "\n",
    "# Remove any rows with NaT in 'DateTime'\n",
    "empatica_data = empatica_data.dropna(subset=['DateTime'])\n",
    "\n",
    "# Floor to seconds to ensure alignment\n",
    "empatica_data['DateTime'] = empatica_data['DateTime'].dt.floor('S')\n",
    "\n",
    "# --- Climate Data Processing ---\n",
    "\n",
    "# Locate the climate data CSV file\n",
    "print(\"Locating climate data CSV file...\")\n",
    "log2_path = os.path.join(sourcedata_path, 'log2')\n",
    "subject_path = os.path.join(log2_path, subject_folder)\n",
    "\n",
    "# Find session folder containing the session name\n",
    "session_folder = None\n",
    "for folder in os.listdir(subject_path):\n",
    "    if session_name in folder:\n",
    "        session_folder = folder\n",
    "        break\n",
    "\n",
    "if session_folder is None:\n",
    "    print(f\"Session folder containing '{session_name}' not found in {subject_path}.\")\n",
    "else:\n",
    "    session_path = os.path.join(subject_path, session_folder)\n",
    "\n",
    "    # Find the CSV file containing 'geodata_processed' in the filename\n",
    "    climate_csv_file = None\n",
    "    for file in os.listdir(session_path):\n",
    "        if 'geodata_processed' in file and file.endswith('.csv'):\n",
    "            climate_csv_file = file\n",
    "            break\n",
    "\n",
    "    if climate_csv_file is None:\n",
    "        print(f\"No CSV file containing 'geodata_processed' found in {session_path}.\")\n",
    "    else:\n",
    "        climate_csv_path = os.path.join(session_path, climate_csv_file)\n",
    "        print(f\"Climate data CSV file found: {climate_csv_path}\")\n",
    "\n",
    "        # Read the climate data CSV file\n",
    "        print(\"Reading climate data...\")\n",
    "        climate_data = pd.read_csv(climate_csv_path)\n",
    "\n",
    "        # Ensure datetime column is parsed correctly\n",
    "        if 'DateTime' in climate_data.columns:\n",
    "            climate_data['DateTime'] = pd.to_datetime(climate_data['DateTime'], errors='coerce')\n",
    "        else:\n",
    "            print(\"No 'DateTime' column found in climate data. Please ensure the CSV contains datetime information.\")\n",
    "            climate_data = None\n",
    "\n",
    "        # Remove any rows with NaT in 'DateTime'\n",
    "        if climate_data is not None:\n",
    "            climate_data = climate_data.dropna(subset=['DateTime'])\n",
    "\n",
    "        # Check if 'utci' column exists\n",
    "        if climate_data is not None and 'utci' not in climate_data.columns:\n",
    "            print(\"'utci' column not found in climate data.\")\n",
    "            climate_data = None\n",
    "\n",
    "        if climate_data is not None:\n",
    "            # Check if 'second' column exists\n",
    "            if 'second' in climate_data.columns:\n",
    "                # Combine 'DateTime' and 'second' to create a full datetime with seconds\n",
    "                print(\"Combining 'DateTime' and 'second' to create full datetime with seconds...\")\n",
    "                # Floor 'DateTime' to the nearest minute\n",
    "                climate_data['DateTime'] = climate_data['DateTime'].dt.floor('min')\n",
    "                # Add 'second' column as timedelta\n",
    "                climate_data['DateTime'] += pd.to_timedelta(climate_data['second'], unit='s')\n",
    "                # Now 'DateTime' includes seconds\n",
    "            else:\n",
    "                print(\"'second' column not found in climate data.\")\n",
    "                climate_data = None\n",
    "\n",
    "            # Ensure 'DateTime' is in datetime format\n",
    "            if climate_data is not None and climate_data['DateTime'].dtype != 'datetime64[ns]':\n",
    "                climate_data['DateTime'] = pd.to_datetime(climate_data['DateTime'], errors='coerce')\n",
    "                climate_data = climate_data.dropna(subset=['DateTime'])\n",
    "\n",
    "            # Check data type\n",
    "            print(\"Climate data 'DateTime' dtype:\", climate_data['DateTime'].dtype)\n",
    "\n",
    "            # Proceed if 'DateTime' and 'utci' are available\n",
    "            if climate_data is not None:\n",
    "                # Merge Empatica data with climate data on 'DateTime'\n",
    "                print(\"Merging Empatica data with climate data...\")\n",
    "                combined_data = pd.merge(empatica_data, climate_data[['DateTime', 'utci']], on='DateTime', how='inner')\n",
    "\n",
    "                # Save the combined data to CSV with proper datetime formatting\n",
    "                combined_data.to_csv(os.path.join(output_dir, 'combined_data.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Perform correlations between 'utci' and Empatica variables\n",
    "                print(\"Performing correlations between 'utci' and Empatica variables...\")\n",
    "                empatica_vars = combined_data.select_dtypes(include=[np.number]).columns.drop('utci')\n",
    "                correlation_results = []\n",
    "\n",
    "                for var in empatica_vars:\n",
    "                    # Drop NaN values for the pair\n",
    "                    valid_data = combined_data[['utci', var]].dropna()\n",
    "                    if len(valid_data) < 2:\n",
    "                        print(f\"Not enough data to compute correlation between 'utci' and '{var}'.\")\n",
    "                        continue\n",
    "\n",
    "                    # Pearson correlation\n",
    "                    pearson_corr, pearson_p = pearsonr(valid_data['utci'], valid_data[var])\n",
    "\n",
    "                    # Spearman correlation\n",
    "                    spearman_corr, spearman_p = spearmanr(valid_data['utci'], valid_data[var])\n",
    "\n",
    "                    # Append results\n",
    "                    correlation_results.append({\n",
    "                        'Variable': var,\n",
    "                        'Pearson_Correlation': pearson_corr,\n",
    "                        'Pearson_p_value': pearson_p,\n",
    "                        'Spearman_Correlation': spearman_corr,\n",
    "                        'Spearman_p_value': spearman_p\n",
    "                    })\n",
    "\n",
    "                    # Plot scatter plot with best-fit line\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "\n",
    "                    # Scatter plot\n",
    "                    plt.scatter(valid_data['utci'], valid_data[var], color='blue', alpha=0.6, edgecolor='k', label='Data')\n",
    "\n",
    "                    # Best-fit line using linear regression (OLS)\n",
    "                    slope, intercept, r_value, p_value, std_err = linregress(valid_data['utci'], valid_data[var])\n",
    "                    x_vals = np.array([valid_data['utci'].min(), valid_data['utci'].max()])\n",
    "                    y_vals = intercept + slope * x_vals\n",
    "                    plt.plot(x_vals, y_vals, color='red', linewidth=2, label='OLS Linear Regression')\n",
    "\n",
    "                    # Aesthetics adjustments\n",
    "                    plt.xlabel('Universal Thermal Climate Index (UTCI)', fontsize=14)\n",
    "                    plt.ylabel(var, fontsize=14)\n",
    "                    plt.title(f'Relationship between {var} and UTCI', fontsize=16)\n",
    "                    plt.legend(frameon=False, fontsize=12)\n",
    "\n",
    "                    # Remove background grid and frame\n",
    "                    plt.gca().spines['top'].set_visible(False)\n",
    "                    plt.gca().spines['right'].set_visible(False)\n",
    "                    plt.gca().spines['left'].set_linewidth(1.5)\n",
    "                    plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "                    # Include Pearson and Spearman correlation coefficients and p-values in the plot\n",
    "                    textstr = '\\n'.join((\n",
    "                        r'Pearson $r=%.2f$ (p=%.2e)' % (pearson_corr, pearson_p),\n",
    "                        r'Spearman $\\rho=%.2f$ (p=%.2e)' % (spearman_corr, spearman_p)\n",
    "                    ))\n",
    "                    # Place text box in upper left in axes coords\n",
    "                    plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=12,\n",
    "                            verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "                    # Tight layout for better spacing\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    # Save the figure\n",
    "                    plt.savefig(os.path.join(output_dir, f'scatter_{var}_vs_utci.png'), dpi=300)\n",
    "                    plt.close()\n",
    "\n",
    "                # Save correlation results to CSV\n",
    "                print(\"Saving correlation results...\")\n",
    "                correlation_df = pd.DataFrame(correlation_results)\n",
    "                correlation_df.to_csv(os.path.join(output_dir, 'correlation_results.csv'), index=False)\n",
    "\n",
    "                print(\"Correlation analysis complete. Results saved to the output directory.\")\n",
    "            else:\n",
    "                print(\"Climate data could not be processed due to missing 'DateTime', 'utci', or 'second' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Covariance Matrices\n",
    "Perform correlations for:\n",
    "- within subjects\n",
    "- within paths\n",
    "- between paths\n",
    "- between subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_covariance_matrix(data, method='pairwise'):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix using different methods to handle missing data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with numeric variables.\n",
    "        method (str): Method to handle missing values. Options are:\n",
    "            - 'pairwise': Computes pairwise covariances, ignoring missing values.\n",
    "            - 'mean_impute': Fills missing values with the column mean.\n",
    "            - 'median_impute': Fills missing values with the column median.\n",
    "            - 'interpolate': Uses linear interpolation to fill missing values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Covariance matrix.\n",
    "    \"\"\"\n",
    "    if method == 'pairwise':\n",
    "        # Pandas computes pairwise covariances by default\n",
    "        return data.cov()\n",
    "    elif method == 'mean_impute':\n",
    "        filled_data = data.fillna(data.mean())\n",
    "    elif method == 'median_impute':\n",
    "        filled_data = data.fillna(data.median())\n",
    "    elif method == 'interpolate':\n",
    "        filled_data = data.interpolate(method='linear', limit_direction='both')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return filled_data.cov()\n",
    "\n",
    "def process_data(empatica_data_path, climate_data_path, output_dir):\n",
    "    \n",
    "    # Load Empatica data\n",
    "    empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "    empatica_data = empatica_data.dropna(subset=['DateTime']).set_index('DateTime')\n",
    "\n",
    "    # Load Climate data\n",
    "    climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "    climate_data['time'] = pd.to_datetime(climate_data['time'], errors='coerce')\n",
    "    climate_data = climate_data.dropna(subset=['time']).set_index('time')\n",
    "\n",
    "    # Merge data\n",
    "    combined_data = pd.merge(empatica_data, climate_data[['utci']], left_index=True, right_index=True, how='inner')\n",
    "    combined_data.to_csv(os.path.join(output_dir, 'combined_data.csv'))\n",
    "\n",
    "    # Select numeric columns for covariance analysis\n",
    "    numeric_data = combined_data.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Compute covariance matrices with different methods\n",
    "    covariance_methods = ['pairwise', 'mean_impute', 'median_impute', 'interpolate']\n",
    "    for method in covariance_methods:\n",
    "        print(f\"Computing covariance matrix using {method} method...\")\n",
    "        covariance_matrix = compute_covariance_matrix(numeric_data, method=method)\n",
    "        covariance_matrix.to_csv(os.path.join(output_dir, f'covariance_matrix_{method}.csv'))\n",
    "        print(f\"Covariance matrix ({method}) saved.\")\n",
    "\n",
    "    print(\"Covariance matrix computations complete.\")\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    session_name = 'Lapa'\n",
    "    subject_folder = 'sub-OE009'\n",
    "    output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, f'ses-{session_name}')\n",
    "    empatica_data_path = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "    climate_data_path = os.path.join(path.sourcedata, 'supp', 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process data and compute covariance matrices\n",
    "    process_data(empatica_data_path, climate_data_path, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, ttest_rel, pearsonr, spearmanr\n",
    "\n",
    "###############################################################################\n",
    "# Choose which columns from geodata to include (besides \"time\") \n",
    "# Typically, you want numeric columns that might be correlated with physiological data.\n",
    "# Exclude obviously irrelevant columns like 'geometry' or duplicates.\n",
    "###############################################################################\n",
    "SELECTED_GEODATA_COLS = [\n",
    "    # Potential environment/climate columns\n",
    "    \"tk_airquality_iaqindex_value\",\"tk_airquality_temperature_value\",\"tk_airquality_humidity_value\",\n",
    "    \"tk_airquality_airpressure_value\",\"tk_soundpressurelevel_spl_value\",\"tk_humidity_humidity_value\",\n",
    "    \"tk_analogin_voltage_value\",\"tk_particulatematter_pm1_0_value\",\"tk_particulatematter_pm2_5_value\",\n",
    "    \"tk_particulatematter_pm10_0_value\",\"tk_dual0_20ma_solarlight_value\",\"tk_thermocouple_temperature_value\",\n",
    "    \"tk_ptc_airtemp_value\",\"atmos_northwind_value\",\"atmos_eastwind_value\",\"atmos_gustwind_value\",\n",
    "    \"atmos_airtemperature_value\",\"accelerometer_orientation_x\",\"accelerometer_orientation_y\",\"accelerometer_orientation_z\",\n",
    "    \"accelerometer_gyroscope_x\",\"accelerometer_gyroscope_y\",\"accelerometer_gyroscope_z\",\"accelerometer_linearaccl_x\",\n",
    "    \"accelerometer_linearaccl_y\",\"accelerometer_linearaccl_z\",\"accelerometer_magnetometer_x\",\"accelerometer_magnetometer_y\",\n",
    "    \"accelerometer_magnetometer_z\",\"accelerometer_accl_x\",\"accelerometer_accl_y\",\"accelerometer_accl_z\",\n",
    "    \"accelerometer_gravity_x\",\"accelerometer_gravity_y\",\"accelerometer_gravity_z\",\n",
    "    # Merged columns from subsequent steps\n",
    "    \"empatica_e4_gsr\",\"empatica_e4_hr\",\"empatica_e4_ibi\",\"empatica_e4_temperature\",\n",
    "    \"humidity\",\"wind_speed\",\"temp_atmos\",\"temp_tk\",\"temp_tk_ptc\",\"temp_radiant\",\"utci\",\n",
    "    \"day_of_year\",\"ghi\",\"hPa\",\"dew_point\",\"solar_altitude\",\"solar_azimuth\",\"mrt\"\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "def compute_covariance_matrix(data, method='pairwise'):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix using different methods to handle missing data.\n",
    "    \"\"\"\n",
    "    if method == 'pairwise':\n",
    "        # Pandas defaults to pairwise complete for .cov()\n",
    "        return data.cov()\n",
    "    elif method == 'mean_impute':\n",
    "        filled_data = data.fillna(data.mean(numeric_only=True))\n",
    "    elif method == 'median_impute':\n",
    "        filled_data = data.fillna(data.median(numeric_only=True))\n",
    "    elif method == 'interpolate':\n",
    "        filled_data = data.interpolate(method='linear', limit_direction='both')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return filled_data.cov()\n",
    "\n",
    "def compute_correlation_matrix(data, method='pearson'):\n",
    "    \"\"\"\n",
    "    Compute correlation matrix: 'pearson', 'spearman', or 'kendall'.\n",
    "    \"\"\"\n",
    "    return data.corr(method=method)\n",
    "\n",
    "def plot_heatmap(matrix, title, output_path):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the given matrix (DataFrame).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(matrix, annot=False, cmap='coolwarm', square=True)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def process_data(empatica_data_path, climate_data_path, output_dir):\n",
    "    \n",
    "    ###############################################################################\n",
    "    # Load Empatica Data\n",
    "    ###############################################################################\n",
    "    empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "    empatica_data['DateTime'] = pd.to_datetime(empatica_data['DateTime'], errors='coerce')\n",
    "    empatica_data.dropna(subset=['DateTime'], inplace=True)\n",
    "    empatica_data.set_index('DateTime', inplace=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Load \"geodata\" / climate data\n",
    "    ###############################################################################\n",
    "    climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "    climate_data['time'] = pd.to_datetime(climate_data['time'], errors='coerce')\n",
    "    climate_data.dropna(subset=['time'], inplace=True)\n",
    "    climate_data.set_index('time', inplace=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Merge\n",
    "    # We'll gather columns from climate_data that we want \n",
    "    # (some may not exist, so we will safely drop missing)\n",
    "    ###############################################################################\n",
    "    available_cols = [c for c in SELECTED_GEODATA_COLS if c in climate_data.columns]\n",
    "    # If none found, we proceed with a minimal set\n",
    "    if not available_cols:\n",
    "        print(\"No selected columns found in climate_data. Falling back to just utci if present.\")\n",
    "        if \"utci\" in climate_data.columns:\n",
    "            available_cols = [\"utci\"]\n",
    "        else:\n",
    "            print(\"No utci in climate data; the merge will produce minimal columns.\")\n",
    "            available_cols = climate_data.columns  # fallback to all\n",
    "\n",
    "    # Subset climate_data\n",
    "    climate_subset = climate_data[available_cols]\n",
    "\n",
    "    # Merge\n",
    "    combined_data = pd.merge(empatica_data, climate_subset, \n",
    "                             left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    # Export the final table\n",
    "    combined_data.to_csv(os.path.join(output_dir, 'combined_data_full.csv'))\n",
    "    print(f\"Exported final combined data with {len(combined_data)} rows, {combined_data.shape[1]} columns.\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # Covariance / Correlation\n",
    "    ###############################################################################\n",
    "    # We'll select numeric columns only\n",
    "    numeric_data = combined_data.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Possibly we drop columns that have zero variance or near-constant\n",
    "    # or too many NaNs, but let's keep it simple\n",
    "\n",
    "    # 1) Covariance with different methods\n",
    "    covariance_methods = ['pairwise', 'mean_impute', 'median_impute', 'interpolate']\n",
    "    for method in covariance_methods:\n",
    "        cov_mat = compute_covariance_matrix(numeric_data, method=method)\n",
    "        cov_mat.to_csv(os.path.join(output_dir, f'cov_matrix_{method}.csv'))\n",
    "        plot_heatmap(cov_mat, f\"Covariance Matrix [{method}]\", \n",
    "                     os.path.join(output_dir, f'cov_matrix_{method}.png'))\n",
    "\n",
    "    # 2) Correlation (Pearson, Spearman)\n",
    "    for corr_method in ['pearson','spearman']:\n",
    "        corr_mat = compute_correlation_matrix(numeric_data, method=corr_method)\n",
    "        corr_mat.to_csv(os.path.join(output_dir, f'corr_matrix_{corr_method}.csv'))\n",
    "        plot_heatmap(corr_mat, f\"Correlation Matrix [{corr_method}]\", \n",
    "                     os.path.join(output_dir, f'corr_matrix_{corr_method}.png'))\n",
    "\n",
    "    print(\"Covariance and correlation matrices saved & plotted.\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # MAIN SCRIPT\n",
    "    datadir = os.path.join(path.sourcedata, 'data')\n",
    "    for participant_folder in os.listdir(datadir):\n",
    "        if participant_folder.startswith(\"OE\"):\n",
    "            participant_path = os.path.join(datadir, participant_folder)\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                session_path = os.path.join(participant_path, session_folder)\n",
    "                if os.path.isdir(session_path):\n",
    "                    session_name = extract_session_name(session_folder)\n",
    "                    subject_folder = f'sub-{participant_folder}'\n",
    "                    output_dir = os.path.join(path.sourcedata, 'supp', 'correlation', subject_folder, f'ses-{session_name}')\n",
    "                    empatica_data_path = os.path.join(path.sourcedata, 'supp', 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "                    climate_data_path = os.path.join(path.sourcedata, 'supp', 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "                    if not os.path.exists(output_dir):    \n",
    "                        os.makedirs(output_dir, exist_ok=True)  \n",
    "                    try:\n",
    "                        print(f\"Processing {participant_folder}-{session_name}...\")\n",
    "                        process_data(empatica_data_path, climate_data_path, output_dir)\n",
    "                    except:\n",
    "                        print(f\"Error in {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from utils import *\n",
    "\n",
    "###############################################################################\n",
    "# Lists for separation of variables\n",
    "# You can adapt these to your actual column names\n",
    "###############################################################################\n",
    "PHYS_VARS = [\n",
    "    \"empatica_e4_gsr\",\"empatica_e4_hr\",\"empatica_e4_ibi\",\"empatica_e4_temperature\",\n",
    "    # add other physiol columns if needed\n",
    "]\n",
    "\n",
    "CLIM_VARS = [\n",
    "    \"tk_airquality_iaqindex_value\",\"tk_airquality_temperature_value\",\"tk_airquality_humidity_value\",\n",
    "    \"tk_airquality_airpressure_value\",\"tk_soundpressurelevel_spl_value\",\"tk_humidity_humidity_value\",\n",
    "    \"tk_analogin_voltage_value\",\"tk_particulatematter_pm1_0_value\",\"tk_particulatematter_pm2_5_value\",\n",
    "    \"tk_particulatematter_pm10_0_value\",\"tk_dual0_20ma_solarlight_value\",\"tk_thermocouple_temperature_value\",\n",
    "    \"tk_ptc_airtemp_value\",\"atmos_northwind_value\",\"atmos_eastwind_value\",\"atmos_gustwind_value\",\n",
    "    \"atmos_airtemperature_value\",\"humidity\",\"wind_speed\",\"temp_atmos\",\"temp_tk\",\"temp_tk_ptc\",\n",
    "    \"temp_radiant\",\"utci\",\"day_of_year\",\"ghi\",\"hPa\",\"dew_point\",\"solar_altitude\",\"solar_azimuth\",\n",
    "    \"mrt\"\n",
    "]\n",
    "\n",
    "def do_within_session_correlations(combined_data, session_name, output_dir):\n",
    "    \"\"\"\n",
    "    For a single session, compute correlation between all PHYS_VARS x CLIM_VARS,\n",
    "    store in a DataFrame, and plot a heatmap.\n",
    "    \"\"\"\n",
    "    # Keep only columns that exist\n",
    "    available_phys = [v for v in PHYS_VARS if v in combined_data.columns]\n",
    "    available_clim = [v for v in CLIM_VARS if v in combined_data.columns]\n",
    "\n",
    "    if not (available_phys and available_clim):\n",
    "        print(f\"No overlapping columns for session={session_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # We can do pairwise correlation row by row\n",
    "    # First drop rows with all NaN in those columns\n",
    "    sub_df = combined_data[available_phys + available_clim].dropna(how='all')\n",
    "\n",
    "    # We'll create a result matrix shaped (len(phys), len(clim)) with correlation\n",
    "    corr_values = np.zeros((len(available_phys), len(available_clim)))\n",
    "    p_values    = np.zeros((len(available_phys), len(available_clim)))\n",
    "\n",
    "    for i, pv in enumerate(available_phys):\n",
    "        for j, cv in enumerate(available_clim):\n",
    "            # Drop row if either is NaN\n",
    "            pair_df = sub_df[[pv, cv]].dropna()\n",
    "            if len(pair_df) > 2:\n",
    "                r, p = pearsonr(pair_df[pv], pair_df[cv])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            corr_values[i,j] = r\n",
    "            p_values[i,j]    = p\n",
    "\n",
    "    # Create a DataFrame\n",
    "    corr_df = pd.DataFrame(corr_values, index=available_phys, columns=available_clim)\n",
    "    pval_df = pd.DataFrame(p_values,  index=available_phys, columns=available_clim)\n",
    "\n",
    "    # Save CSV\n",
    "    corr_df.to_csv(os.path.join(output_dir, f\"{session_name}_phys_clim_correlations.csv\"))\n",
    "    pval_df.to_csv(os.path.join(output_dir, f\"{session_name}_phys_clim_pvalues.csv\"))\n",
    "\n",
    "    # Plot heatmap of correlation (r-values)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(f\"PHYS-CLIM Correlations (Pearson R)\\nSession: {session_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{session_name}_phys_clim_correlation_heatmap.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Session={session_name}: correlation done. Saved to {output_dir}\")\n",
    "\n",
    "def do_between_sessions_correlations(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Compute correlation for all sessions combined (pooled data).\n",
    "    We'll do the same approach as do_within_session_correlations,\n",
    "    but on the entire dataset from all sessions.\n",
    "    \"\"\"\n",
    "    available_phys = [v for v in PHYS_VARS if v in all_data.columns]\n",
    "    available_clim = [v for v in CLIM_VARS if v in all_data.columns]\n",
    "\n",
    "    if not (available_phys and available_clim):\n",
    "        print(\"No overlapping columns for the entire dataset. Skipping between-sessions correlation.\")\n",
    "        return\n",
    "\n",
    "    # Drop rows with no data in those columns\n",
    "    sub_df = all_data[available_phys + available_clim].dropna(how='all')\n",
    "\n",
    "    corr_values = np.zeros((len(available_phys), len(available_clim)))\n",
    "    p_values    = np.zeros((len(available_phys), len(available_clim)))\n",
    "\n",
    "    for i, pv in enumerate(available_phys):\n",
    "        for j, cv in enumerate(available_clim):\n",
    "            pair_df = sub_df[[pv, cv]].dropna()\n",
    "            if len(pair_df) > 2:\n",
    "                r, p = pearsonr(pair_df[pv], pair_df[cv])\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "            corr_values[i,j] = r\n",
    "            p_values[i,j]    = p\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_values, index=available_phys, columns=available_clim)\n",
    "    pval_df = pd.DataFrame(p_values,  index=available_phys, columns=available_clim)\n",
    "\n",
    "    corr_df.to_csv(os.path.join(output_dir, \"ALL_physiol_clim_correlations.csv\"))\n",
    "    pval_df.to_csv(os.path.join(output_dir, \"ALL_physiol_clim_pvalues.csv\"))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(\"PHYS-CLIM Correlations (Pearson R)\\nAll Sessions Combined\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"ALL_physiol_clim_correlation_heatmap.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Between-sessions correlation done. Saved to output folder.\")\n",
    "\n",
    "def main():\n",
    "    # We'll do something similar to your existing approach:\n",
    "    # We iterate over participants/sessions. For each session, we merge data, store in memory, do correlation, etc.\n",
    "\n",
    "    # Example session map if needed\n",
    "    sessions_map = {\n",
    "       'Baixa': 4,\n",
    "       'Belem': 1,\n",
    "       'Parque': 6,\n",
    "       'Gulbenkian': 3,\n",
    "       'Lapa': 2,\n",
    "       'Graca': 5\n",
    "    }\n",
    "\n",
    "    # We'll keep an all_data list to do the between-sessions approach\n",
    "    all_data_frames = []\n",
    "    search_dir = os.path.join(path.sourcedata, 'data')\n",
    "    datadir = os.path.join(path.sourcedata, 'supp')\n",
    "    for participant_folder in os.listdir(search_dir):\n",
    "        if participant_folder.startswith(\"OE\"):\n",
    "            participant_path = os.path.join(search_dir, participant_folder)\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                session_path = os.path.join(participant_path, session_folder)\n",
    "                if os.path.isdir(session_path):\n",
    "                    session_name = extract_session_name(session_folder)\n",
    "                    # Build the paths\n",
    "                    subject_folder = f'sub-{participant_folder}'\n",
    "                    output_dir = os.path.join(datadir, \"correlation\", subject_folder, f'ses-{session_name}')\n",
    "                    empatica_data_path = os.path.join(datadir, 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "                    climate_data_path  = os.path.join(datadir, 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                    # 1) Merge\n",
    "                    #   We'll do a simplified approach here or adapt from your existing \"process_data\" function\n",
    "                    try:\n",
    "                        # Load Empatica\n",
    "                        empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "                        empatica_data.dropna(subset=['DateTime'], inplace=True)\n",
    "                        empatica_data.set_index('DateTime', inplace=True)\n",
    "\n",
    "                        # Load climate\n",
    "                        climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "                        climate_data.dropna(subset=['time'], inplace=True)\n",
    "                        climate_data.set_index('time', inplace=True)\n",
    "\n",
    "                        # Merge\n",
    "                        combined_data = pd.merge(empatica_data, climate_data, left_index=True, right_index=True, how='inner')\n",
    "                        # Possibly keep only numeric columns\n",
    "                        # But let's keep all columns for referencing\n",
    "                        combined_data.to_csv(os.path.join(output_dir, 'merged.csv'))\n",
    "                        \n",
    "                        # 2) Within-Session correlation\n",
    "                        do_within_session_correlations(combined_data, session_name, output_dir)\n",
    "\n",
    "                        # Keep for all-data approach\n",
    "                        # We'll add a column 'Session' so we know from which session each row is\n",
    "                        combined_data[\"Session\"] = session_name\n",
    "                        all_data_frames.append(combined_data)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error merging or analyzing {participant_folder}-{session_name}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    # 3) Between-Session correlation\n",
    "    if all_data_frames:\n",
    "        big_df = pd.concat(all_data_frames, axis=0, ignore_index=False)\n",
    "        # We'll do one big correlation\n",
    "        out_all_dir = os.path.join(datadir, \"correlation\", \"ALL_SESSIONS\")\n",
    "        os.makedirs(out_all_dir, exist_ok=True)\n",
    "        do_between_sessions_correlations(big_df, out_all_dir)\n",
    "    else:\n",
    "        print(\"No data frames combined. Possibly no merges succeeded.\")\n",
    "\n",
    "    print(\"Done entire pipeline.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "PHYS_VARS = [\n",
    "    \"empatica_e4_gsr\",\"empatica_e4_hr\",\"empatica_e4_ibi\",\"empatica_e4_temperature\",\n",
    "    # add more if needed\n",
    "]\n",
    "\n",
    "CLIM_VARS = [\n",
    "    \"tk_airquality_iaqindex_value\",\"tk_airquality_temperature_value\",\"tk_airquality_humidity_value\",\n",
    "    \"tk_airquality_airpressure_value\",\"tk_soundpressurelevel_spl_value\",\"tk_humidity_humidity_value\",\n",
    "    \"tk_analogin_voltage_value\",\"tk_particulatematter_pm1_0_value\",\"tk_particulatematter_pm2_5_value\",\n",
    "    \"tk_particulatematter_pm10_0_value\",\"tk_dual0_20ma_solarlight_value\",\"tk_thermocouple_temperature_value\",\n",
    "    \"tk_ptc_airtemp_value\",\"atmos_northwind_value\",\"atmos_eastwind_value\",\"atmos_gustwind_value\",\n",
    "    \"atmos_airtemperature_value\",\"humidity\",\"wind_speed\",\"temp_atmos\",\"temp_tk\",\"temp_tk_ptc\",\n",
    "    \"temp_radiant\",\"utci\",\"day_of_year\",\"ghi\",\"hPa\",\"dew_point\",\"solar_altitude\",\"solar_azimuth\",\n",
    "    \"mrt\"\n",
    "]\n",
    "\n",
    "def accumulate_session_data(session_data_frames, session_name, combined_data):\n",
    "    \"\"\"\n",
    "    Accumulate data for each session into a dictionary of lists. \n",
    "    We'll later concatenate them and compute one correlation matrix per session.\n",
    "    \"\"\"\n",
    "    if session_name not in session_data_frames:\n",
    "        session_data_frames[session_name] = []\n",
    "    session_data_frames[session_name].append(combined_data)\n",
    "\n",
    "def compute_session_correlation(df_session, session_name, outdir):\n",
    "    \"\"\"\n",
    "    For the pooled data in a single session, compute correlation \n",
    "    among PHYS_VARS x CLIM_VARS and return the correlation DataFrame.\n",
    "    \"\"\"\n",
    "    available_phys = [v for v in PHYS_VARS if v in df_session.columns]\n",
    "    available_clim = [v for v in CLIM_VARS if v in df_session.columns]\n",
    "\n",
    "    if not (available_phys and available_clim):\n",
    "        print(f\"No overlapping columns for session={session_name}. No correlation computed.\")\n",
    "        return None\n",
    "\n",
    "    # Subset numeric columns\n",
    "    sub_df = df_session[available_phys + available_clim].dropna(how='all')\n",
    "    # Create a matrix (len(phys) x len(clim)) for correlation\n",
    "    corr_values = np.zeros((len(available_phys), len(available_clim)))\n",
    "\n",
    "    for i, pv in enumerate(available_phys):\n",
    "        for j, cv in enumerate(available_clim):\n",
    "            pair_df = sub_df[[pv, cv]].dropna()\n",
    "            if len(pair_df) > 2:\n",
    "                r, _ = pearsonr(pair_df[pv], pair_df[cv])\n",
    "            else:\n",
    "                r = np.nan\n",
    "            corr_values[i,j] = r\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_values, index=available_phys, columns=available_clim)\n",
    "\n",
    "    # Save CSV\n",
    "    corr_df.to_csv(os.path.join(outdir, f\"{session_name}_PHYS_CLIM_correlation.csv\"))\n",
    "    return corr_df\n",
    "\n",
    "def main():\n",
    "    # Example session names: 'Baixa', 'Belem', 'Parque', 'Gulbenkian', 'Lapa', 'Graca'\n",
    "    # We'll store data per session for pooling:\n",
    "    session_data_frames = {}  # e.g. { 'Lapa': [df1, df2,...], 'Parque': [...], ... }\n",
    "\n",
    "    # We'll also keep a global list for \"all sessions\" if needed\n",
    "    all_data_frames = []\n",
    "\n",
    "    # Example search dir\n",
    "    search_dir = os.path.join(path.sourcedata, 'data')\n",
    "    datadir = os.path.join(path.sourcedata, 'supp')\n",
    "    for participant_folder in os.listdir(search_dir):\n",
    "        if participant_folder.startswith(\"OE\"):\n",
    "            participant_path = os.path.join(search_dir, participant_folder)\n",
    "            for session_folder in os.listdir(participant_path):\n",
    "                session_path = os.path.join(participant_path, session_folder)\n",
    "                if os.path.isdir(session_path):\n",
    "                    session_name = extract_session_name(session_folder)\n",
    "                    if session_name is None:\n",
    "                        continue\n",
    "\n",
    "                    # Build hypothetical paths\n",
    "                    subject_folder = f'sub-{participant_folder}'\n",
    "                    empatica_data_path = os.path.join(datadir, 'stress_csv', subject_folder, f'ses-{session_name}', '_1hz', 'data_all_1Hz.csv')\n",
    "                    climate_data_path  = os.path.join(datadir, 'geodata','log', subject_folder, f'ses-{session_name}', f'{subject_folder}_ses-{session_name}_geodata.xlsx')\n",
    "\n",
    "                    if not os.path.exists(empatica_data_path) or not os.path.exists(climate_data_path):\n",
    "                        print(f\"Missing data for {participant_folder}-{session_name}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # Load Empatica\n",
    "                        empatica_data = pd.read_csv(empatica_data_path, parse_dates=['DateTime'])\n",
    "                        empatica_data.dropna(subset=['DateTime'], inplace=True)\n",
    "                        empatica_data.set_index('DateTime', inplace=True)\n",
    "\n",
    "                        # Load climate\n",
    "                        climate_data = pd.read_excel(climate_data_path, parse_dates=['time'])\n",
    "                        climate_data.dropna(subset=['time'], inplace=True)\n",
    "                        climate_data.set_index('time', inplace=True)\n",
    "\n",
    "                        # Merge\n",
    "                        combined_data = pd.merge(empatica_data, climate_data,\n",
    "                                                 left_index=True, right_index=True, how='inner')\n",
    "                        # Add a 'Session' column\n",
    "                        combined_data[\"Session\"] = session_name\n",
    "\n",
    "                        # Accumulate for session-level\n",
    "                        accumulate_session_data(session_data_frames, session_name, combined_data)\n",
    "                        # Accumulate for all-sessions\n",
    "                        all_data_frames.append(combined_data)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error merging {participant_folder}-{session_name}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Now we create an output directory to store session correlations \n",
    "    # (and the final multi-plot)\n",
    "    # ---------------------------------------------------------------------\n",
    "    outdir = os.path.join(datadir, \"all_sessions_correlations\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # For each session, combine data from all participants\n",
    "    # then compute correlation => store in a dictionary\n",
    "    session_corrs = {}\n",
    "    for ses, list_of_dfs in session_data_frames.items():\n",
    "        if not list_of_dfs:\n",
    "            continue\n",
    "        big_ses_df = pd.concat(list_of_dfs, ignore_index=False)\n",
    "        corr_df = compute_session_correlation(big_ses_df, ses, outdir)\n",
    "        if corr_df is not None:\n",
    "            session_corrs[ses] = corr_df\n",
    "\n",
    "    # Now we create a single figure with one subplot for each session\n",
    "    # Suppose we have up to 6 sessions, we can arrange them in (2,3) grid\n",
    "    sessions_sorted = sorted(session_corrs.keys())  # sort alphabetically or otherwise\n",
    "    n_ses = len(sessions_sorted)\n",
    "\n",
    "    fig_cols = 3 if n_ses>3 else n_ses\n",
    "    fig_rows = int(np.ceil(n_ses/fig_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(fig_rows, fig_cols, figsize=(6*fig_cols,5*fig_rows))\n",
    "    axes = np.atleast_2d(axes)  # ensure 2D for indexing\n",
    "\n",
    "    for i, ses in enumerate(sessions_sorted):\n",
    "        row = i // fig_cols\n",
    "        col = i % fig_cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        corr_df = session_corrs[ses]\n",
    "        sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=ax)\n",
    "        ax.set_title(f\"Session: {ses}\", fontsize=12)\n",
    "\n",
    "    # If there's any leftover subplot, hide it\n",
    "    if n_ses < fig_rows*fig_cols:\n",
    "        for empty_i in range(n_ses, fig_rows*fig_cols):\n",
    "            er = empty_i//fig_cols\n",
    "            ec = empty_i%fig_cols\n",
    "            axes[er,ec].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"PHYS-CLIM Correlation Matrices by Session\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    plt.savefig(os.path.join(outdir, \"multi_session_correlations.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Optionally do the \"between-sessions\" correlation \n",
    "    # by pooling all data from all_data_frames\n",
    "    if all_data_frames:\n",
    "        big_df = pd.concat(all_data_frames, ignore_index=False)\n",
    "        # We'll do the same approach as 'compute_session_correlation' \n",
    "        # but call it \"ALL\"\n",
    "        all_corr_df = compute_session_correlation(big_df, \"ALL\", outdir)\n",
    "        if all_corr_df is not None:\n",
    "            plt.figure(figsize=(12,6))\n",
    "            sns.heatmap(all_corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "            plt.title(\"PHYS-CLIM Correlation (All Sessions Pooled)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(outdir, \"ALL_sessions_correlation_heatmap.png\"), dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "    print(\"Done. Multi-subplot correlation figure created for each session.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
