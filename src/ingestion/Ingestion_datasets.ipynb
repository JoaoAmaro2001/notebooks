{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "from utils import *\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data\\OE009\\Lisbon_Lapa_sub-OE102009_2024-05-02T132016Z'\n",
    "datapicker = create_datapicker(path = data_path, schema=build_schema)\n",
    "dataset    = load_dataset(datapicker.selected_path,schema=build_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create and export geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import re\n",
    "import utils.for_setpath as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\", \"Radiant Temperature\"])\n",
    "\n",
    "# Directories\n",
    "sourcedata = os.path.join(path.sourcedata, 'data')\n",
    "logdata    = os.path.join(path.sourcedata, 'supp','geodata','log')\n",
    "shpdata    = os.path.join(path.sourcedata, 'supp','interexperimentalpaths_shp')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        data_path  = session_path\n",
    "        datapicker = create_datapicker(path = data_path, schema=build_schema)\n",
    "        dataset    = load_dataset(datapicker.selected_path,schema=build_schema)\n",
    "        status     = 1  # Success\n",
    "        \n",
    "        # Create geodata\n",
    "        geodata = dataset.to_geoframe()\n",
    "\n",
    "        # Process geodata\n",
    "        geodata['time'] = geodata.index.to_pydatetime()\n",
    "        geodata         = tidy_geodata(geodata)\n",
    "        geodata         = add_environmental_metrics(geodata)\n",
    "\n",
    "        # Prepare outputs for the log directory\n",
    "        log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "        gps_file     = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_gps.png\")\n",
    "\n",
    "        # Get path information\n",
    "        path_num = fetch_path_num(data_path)\n",
    "\n",
    "        try: # Fetch correct GPS data and plot it\n",
    "            if path_num == '01':\n",
    "                shp_filename = \"01_belem.shp\"\n",
    "            elif path_num == '02':\n",
    "                shp_filename = \"02_lapa.shp\"\n",
    "            elif path_num == '03':\n",
    "                shp_filename = \"03_gulbenkian.shp\"\n",
    "            elif path_num == '04':\n",
    "                shp_filename = \"04_Baixa.shp\"\n",
    "            elif path_num == '05':\n",
    "                shp_filename = \"05_Graca.shp\"\n",
    "            elif path_num == '06':\n",
    "                shp_filename = \"06_Pnacoes.shp\"\n",
    "            elif path_num == '07':\n",
    "                shp_filename = \"07_ANovas_Sa_Bandeira.shp\"\n",
    "            elif path_num == '08':\n",
    "                shp_filename = \"08_ANovas_CMoeda.shp\"\n",
    "            elif path_num == '09':\n",
    "                shp_filename = \"09_PFranca_Escolas.shp\"\n",
    "            elif path_num == '10':\n",
    "                shp_filename = \"10_PFranca_Morais_Soares.shp\"\n",
    "            elif path_num == '11':\n",
    "                shp_filename = \"11_Marvila_Beato.shp\"\n",
    "            elif path_num == '12':\n",
    "                shp_filename = \"12_PNacoes_Gare.shp\"\n",
    "            elif path_num == '13':\n",
    "                shp_filename = \"13_Madredeus.shp\"\n",
    "            elif path_num == '14':\n",
    "                shp_filename = \"14_Benfica_Pupilos.shp\"\n",
    "            elif path_num == '15':\n",
    "                shp_filename = \"15_Benfica_Moinhos.shp\"\n",
    "            elif path_num == '16':\n",
    "                shp_filename = \"16_Benfica_Grandella.shp\"\n",
    "            elif path_num == '17':\n",
    "                shp_filename = \"17_Restauradores.shp\"\n",
    "            elif path_num == '18':\n",
    "                shp_filename = \"18_Belem_Estadio.shp\"\n",
    "            elif path_num == '19':\n",
    "                shp_filename = \"19_Estrela_Jardim.shp\"\n",
    "            elif path_num == '20':\n",
    "                shp_filename = \"20_Estrela_Assembleia.shp\"\n",
    "            elif path_num == '21':\n",
    "                shp_filename = \"21_Estrela_Rato.shp\"\n",
    "            elif path_num == '22':\n",
    "                shp_filename = \"22_Estrela_Prazeres.shp\"\n",
    "\n",
    "            # Correct GPS data\n",
    "            shp_file        = os.path.join(shpdata, shp_filename)\n",
    "            geodata         = correct_gps_data(geodata, shp_file)\n",
    "            # Plot and save geodata's GPS\n",
    "            plot_save_gps(geodata, shp_file, gps_file)\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for participant '{participant_name}', session '{session_name}': {e}\")\n",
    "            print(\"Most likely the path number is higher than 6\")\n",
    "\n",
    "        # Check if the radiant temperature column exists and contains non-zero values\n",
    "        radiant_temp_status = 0  # Default to 0\n",
    "        if 'tk_thermocouple_temperature_value' in geodata.columns:\n",
    "            if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "                radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "        # Ensure geodata is a DataFrame and save as Excel\n",
    "        if not isinstance(geodata, pd.DataFrame):\n",
    "            geodata = pd.DataFrame(geodata)\n",
    "        geodata.to_excel(geodata_file, index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for participant '{participant_name}', session '{session_name}': {e}\")\n",
    "        print(\"Most likely needs to have an updated EEG .nedf file\")\n",
    "        status = 0  # Failed\n",
    "        radiant_temp_status = 0  # Default to 0\n",
    "\n",
    "    # Log the result in the Excel sheet\n",
    "    ws.append([participant_name, session_name, status, radiant_temp_status])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "for participant_folder in os.listdir(sourcedata):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(sourcedata, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                session_name = extract_session_name(session_folder)\n",
    "                process_session(session_path, participant_folder, session_name)\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "wb.save(result_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import re\n",
    "import utils.for_setpath as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "\n",
    "geodata = dataset.to_geoframe()\n",
    "# Process geodata\n",
    "geodata['time'] = geodata.index.to_pydatetime()\n",
    "geodata         = tidy_geodata(geodata)\n",
    "geodata         = add_environmental_metrics(geodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Bidify Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Get log data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\"])\n",
    "\n",
    "# Path information\n",
    "sourcedata = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data'\n",
    "bidsroot = r'Z:\\Exp_4-outdoor_walk\\lisbon\\bids'\n",
    "\n",
    "# Save the Excel file\n",
    "wb.save(\"session_processing_results.xlsx\")\n",
    "\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    data_path = os.path.join(session_path)\n",
    "    try:\n",
    "        datapicker.reset(path=data_path)\n",
    "        create_dataset(datapicker=datapicker)\n",
    "        status = \"Success\"\n",
    "    except Exception as e:\n",
    "        status = f\"Failed: {str(e)}\"\n",
    "    \n",
    "    # Log the result\n",
    "    ws.append([participant_name, session_name, status])\n",
    "    \n",
    "    if status == \"Success\":\n",
    "        # Generate and bidify the geodata\n",
    "        geodata = datapicker.geodata # do not use datapicker.geodata()\n",
    "        create_geodata(geodata)\n",
    "        bidify_geodata(geodata, participant_name, session_name)\n",
    "\n",
    "def create_geodata(geodata):\n",
    "    \"\"\"Compute UTCI across whole time series.\"\"\"\n",
    "    \n",
    "    # Define custom parameters\n",
    "    humidity = geodata.tk_humidity_humidity_value / 100  # in fraction\n",
    "    wind_speed = np.sqrt(geodata.atmos_northwind_value**2 + geodata.atmos_eastwind_value**2)  # m/s (~2.5 m of elevation)\n",
    "    temp_atmos = geodata.atmos_airtemperature_value  # in ºC\n",
    "    temp_tk = geodata.tk_airquality_temperature_value / 100  # in ºC\n",
    "    temp_tk_ptc = geodata.tk_ptc_airtemp_value / 100  # in ºC\n",
    "    temp_radiant = geodata.tk_thermocouple_temperature_value / 100  # in ºC\n",
    "\n",
    "    # Assign custom parameters to the geodata attribute\n",
    "    geodata['humidity'] = humidity\n",
    "    geodata['wind_speed'] = wind_speed\n",
    "    geodata['temp_atmos'] = temp_atmos\n",
    "    geodata['temp_tk'] = temp_tk\n",
    "    geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "    geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "    # Compute the UTCI\n",
    "    geodata['utci'] = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "\n",
    "def bidify_geodata(geodata, participant_name, session_name):\n",
    "    \"\"\"Create the sessions.tsv file in BIDS format and a sidecar JSON file.\"\"\"\n",
    "    \n",
    "    # Replace missing values with 'n/a'\n",
    "    geodata = geodata.fillna('n/a')\n",
    "    \n",
    "    # Prepare the sessions.tsv DataFrame\n",
    "    sessions_df = pd.DataFrame({\n",
    "        'session_id': [f\"ses-{session_name}\"],\n",
    "        'acq_time': [geodata['time'].iloc[0]],  # Use the first value in the 'time' column for acq_time\n",
    "    })\n",
    "\n",
    "    # Include all other columns from geodata in the sessions_df\n",
    "    additional_columns = geodata.iloc[0].to_dict()  # Take the first row as the representative for additional columns\n",
    "    sessions_df = sessions_df.assign(**additional_columns)\n",
    "\n",
    "    # Define the BIDS folder path\n",
    "    bids_folder = os.path.join(bidsroot, f\"sub-{participant_name}\")\n",
    "    os.makedirs(bids_folder, exist_ok=True)\n",
    "\n",
    "    # Save the sessions.tsv file\n",
    "    sessions_file = os.path.join(bids_folder, 'sessions.tsv')\n",
    "    sessions_df.to_csv(sessions_file, sep='\\t', index=False)\n",
    "\n",
    "    # Create the sidecar JSON file\n",
    "    create_sessions_json(bids_folder)\n",
    "\n",
    "def create_sessions_json(bids_folder):\n",
    "    \"\"\"Create the sessions.json file with descriptions for each column.\"\"\"\n",
    "    \n",
    "    sidecar_description = {\n",
    "        \"session_id\": \"A session identifier representing the name of the path.\",\n",
    "        \"acq_time\": \"Acquisition time of the first data point in the session.\",\n",
    "        \"time\": \"Timestamp of each recorded data point in the session, representing the exact time when the measurement was taken.\",\n",
    "        \"tk_gps_data_value\": \"GPS data value, likely representing latitude, longitude, or altitude information collected from the GPS sensor.\",\n",
    "        \"tk_gps_time_value\": \"Timestamp from the GPS sensor, providing the exact time the GPS data was recorded.\",\n",
    "        \"tk_airquality_iaqindex_value\": \"Indoor Air Quality (IAQ) index value, representing the overall air quality based on multiple environmental factors.\",\n",
    "        \"tk_airquality_temperature_value\": \"Temperature reading from the air quality sensor, provided in raw format before conversion to degrees Celsius (°C).\",\n",
    "        \"tk_airquality_humidity_value\": \"Relative humidity as recorded by the air quality sensor, expressed as a percentage.\",\n",
    "        \"tk_airquality_airpressure_value\": \"Air pressure reading from the air quality sensor, measured in hectopascals (hPa).\",\n",
    "        \"tk_soundpressurelevel_spl_value\": \"Sound Pressure Level (SPL) value measured in decibels (dB), indicating the intensity of sound in the environment.\",\n",
    "        \"tk_humidity_humidity_value\": \"Humidity level measured by the TK sensor, expressed as a percentage.\",\n",
    "        \"tk_analogin_voltage_value\": \"Analog input voltage value, possibly representing a sensor's electrical output in volts.\",\n",
    "        \"tk_particulatematter_pm1_0_value\": \"Concentration of particulate matter (PM1.0) in the air, measured in micrograms per cubic meter (µg/m³).\",\n",
    "        \"tk_particulatematter_pm2_5_value\": \"Concentration of particulate matter (PM2.5) in the air, measured in micrograms per cubic meter (µg/m³).\",\n",
    "        \"tk_particulatematter_pm10_0_value\": \"Concentration of particulate matter (PM10) in the air, measured in micrograms per cubic meter (µg/m³).\",\n",
    "        \"tk_dual0_20ma_solarlight_value\": \"Solar light intensity value, measured in raw sensor output (possibly milliamps or another unit related to solar radiation).\",\n",
    "        \"tk_thermocouple_temperature_value\": \"Raw temperature reading from the thermocouple sensor, before conversion to degrees Celsius (°C).\",\n",
    "        \"tk_ptc_airtemp_value\": \"Temperature measured by the Positive Temperature Coefficient (PTC) sensor, in degrees Celsius (°C).\",\n",
    "        \"atmos_northwind_value\": \"Wind speed component in the northward direction, measured in meters per second (m/s).\",\n",
    "        \"atmos_eastwind_value\": \"Wind speed component in the eastward direction, measured in meters per second (m/s).\",\n",
    "        \"atmos_gustwind_value\": \"Maximum wind gust speed recorded during the session, measured in meters per second (m/s).\",\n",
    "        \"atmos_airtemperature_value\": \"Atmospheric temperature during the session, measured in degrees Celsius (°C).\",\n",
    "        \"atmos_xorientation_value\": \"Orientation value in the X-axis, representing the angle of the sensor in the horizontal plane.\",\n",
    "        \"atmos_yorientation_value\": \"Orientation value in the Y-axis, representing the angle of the sensor in the vertical plane.\",\n",
    "        \"atmos_nullvalue_value\": \"A placeholder or null value, typically used to indicate the absence of valid data.\",\n",
    "        \"accelerometer_orientation_x\": \"Orientation value along the X-axis recorded by the accelerometer, representing the tilt or angle of the sensor.\",\n",
    "        \"accelerometer_orientation_y\": \"Orientation value along the Y-axis recorded by the accelerometer, representing the tilt or angle of the sensor.\",\n",
    "        \"accelerometer_orientation_z\": \"Orientation value along the Z-axis recorded by the accelerometer, representing the tilt or angle of the sensor.\",\n",
    "        \"accelerometer_gyroscope_x\": \"Angular velocity around the X-axis recorded by the gyroscope, measured in degrees per second (°/s).\",\n",
    "        \"accelerometer_gyroscope_y\": \"Angular velocity around the Y-axis recorded by the gyroscope, measured in degrees per second (°/s).\",\n",
    "        \"accelerometer_gyroscope_z\": \"Angular velocity around the Z-axis recorded by the gyroscope, measured in degrees per second (°/s).\",\n",
    "        \"accelerometer_linearaccl_x\": \"Linear acceleration along the X-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_linearaccl_y\": \"Linear acceleration along the Y-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_linearaccl_z\": \"Linear acceleration along the Z-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_magnetometer_x\": \"Magnetic field strength along the X-axis recorded by the magnetometer, measured in microteslas (µT).\",\n",
    "        \"accelerometer_magnetometer_y\": \"Magnetic field strength along the Y-axis recorded by the magnetometer, measured in microteslas (µT).\",\n",
    "        \"accelerometer_magnetometer_z\": \"Magnetic field strength along the Z-axis recorded by the magnetometer, measured in microteslas (µT).\",\n",
    "        \"accelerometer_accl_x\": \"Acceleration along the X-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_accl_y\": \"Acceleration along the Y-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_accl_z\": \"Acceleration along the Z-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_gravity_x\": \"Gravity component along the X-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_gravity_y\": \"Gravity component along the Y-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"accelerometer_gravity_z\": \"Gravity component along the Z-axis recorded by the accelerometer, measured in meters per second squared (m/s²).\",\n",
    "        \"humidity\": \"Humidity level during the session, measured as a percentage relative to the maximum amount of moisture air can hold at the given temperature.\",\n",
    "        \"wind_speed\": \"Calculated wind speed considering both northward and eastward components, measured in meters per second (m/s).\",\n",
    "        \"temp_atmos\": \"Atmospheric temperature during the session, measured in degrees Celsius (°C).\",\n",
    "        \"temp_tk\": \"Temperature measured by the TK sensor, converted to degrees Celsius (°C).\",\n",
    "        \"temp_tk_ptc\": \"Temperature measured by the Positive Temperature Coefficient (PTC) sensor, in degrees Celsius (°C).\",\n",
    "        \"temp_radiant\": \"Radiant temperature measured by the thermocouple sensor, in degrees Celsius (°C).\",\n",
    "        \"utci\": \"Universal Thermal Climate Index calculated during the session, which is a measure of the perceived temperature accounting for the effects of wind, humidity, and radiant heat on the human body.\",\n",
    "        \"x\": \"Coordinate or value related to the X-axis in a 3D spatial context.\",\n",
    "        \"y\": \"Coordinate or value related to the Y-axis in a 3D spatial context.\",\n",
    "        \"z\": \"Coordinate or value related to the Z-axis in a 3D spatial context.\"\n",
    "    }\n",
    "\n",
    "    sidecar_file = os.path.join(bids_folder, 'sessions.json')\n",
    "    with open(sidecar_file, 'w') as json_file:\n",
    "        json.dump(sidecar_description, json_file, indent=4)\n",
    "\n",
    "# MAIN SCRIPT\n",
    "\n",
    "# Replace 'OE031' with the specific participant folder you want to test\n",
    "participant_folder = 'OE011'\n",
    "participant_path = os.path.join(sourcedata, participant_folder)\n",
    "\n",
    "for session_folder in os.listdir(participant_path):\n",
    "    session_path = os.path.join(participant_path, session_folder)\n",
    "    if os.path.isdir(session_path):\n",
    "        process_session(session_path, participant_folder, session_folder)\n",
    "\n",
    "# Save the Excel file with the updated status\n",
    "wb.save(\"session_processing_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full loop and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\", \"Radiant Temperature\"])\n",
    "\n",
    "# Path information\n",
    "sourcedata = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data'\n",
    "logdata    = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\supp\\log'\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    data_path = os.path.join(session_path)\n",
    "    try:\n",
    "        datapicker.reset(path=data_path)\n",
    "        create_dataset(datapicker=datapicker)\n",
    "        status = 1  # Success\n",
    "    except Exception as e:\n",
    "        status = 0  # Failed\n",
    "    \n",
    "    radiant_temp_status = 0  # Default to 0 if not found or if all zeros\n",
    "\n",
    "    if status == 1:\n",
    "        # Generate the sessions.tsv file\n",
    "        geodata = datapicker.geodata  # Retrieve geodata\n",
    "        create_geodata(geodata)\n",
    "\n",
    "        # Check if the radiant temperature column exists and contains non-zero values\n",
    "        if 'tk_thermocouple_temperature_value' in geodata:\n",
    "            if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "                radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "        # Save the full geodata to an Excel file in the log directory\n",
    "        log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "        \n",
    "        # Convert geodata to DataFrame if it's not already and save as Excel\n",
    "        if not isinstance(geodata, pd.DataFrame):\n",
    "            geodata = pd.DataFrame(geodata)\n",
    "        geodata.to_excel(geodata_file, index=False)\n",
    "    \n",
    "    # Log the result\n",
    "    ws.append([participant_name, session_name, status, radiant_temp_status])\n",
    "\n",
    "def create_geodata(geodata):\n",
    "    \"\"\"Compute UTCI across whole time series.\"\"\"\n",
    "    \n",
    "    # Define custom parameters\n",
    "    humidity = geodata['tk_humidity_humidity_value'] / 100  # in fraction\n",
    "    wind_speed = np.sqrt(geodata['atmos_northwind_value']**2 + geodata['atmos_eastwind_value']**2)  # m/s (~2.5 m of elevation)\n",
    "    temp_atmos = geodata['atmos_airtemperature_value']  # in ºC\n",
    "    temp_tk = geodata['tk_airquality_temperature_value'] / 100  # in ºC\n",
    "    temp_tk_ptc = geodata['tk_ptc_airtemp_value'] / 100  # in ºC\n",
    "    temp_radiant = geodata['tk_thermocouple_temperature_value'] / 100  # in ºC\n",
    "\n",
    "    # Assign custom parameters to the geodata attribute\n",
    "    geodata['humidity'] = humidity\n",
    "    geodata['wind_speed'] = wind_speed\n",
    "    geodata['temp_atmos'] = temp_atmos\n",
    "    geodata['temp_tk'] = temp_tk\n",
    "    geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "    geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "    # Compute the UTCI\n",
    "    geodata['utci'] = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "\n",
    "    # Get GPS coordinates and integrate them into geodata\n",
    "    coords = datapicker.geodata.geometry.get_coordinates(include_z=True)\n",
    "    # Optionally rename the coordinate columns\n",
    "    coords.rename(columns={'y': 'latitude', 'x': 'longitude', 'z': 'elevation'}, inplace=True)\n",
    "    geodata = geodata.join(coords).drop(columns=['geometry'])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "for participant_folder in os.listdir(sourcedata):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(sourcedata, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                process_session(session_path, participant_folder, session_folder)\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "wb.save(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\", \"Radiant Temperature\"])\n",
    "\n",
    "# Path information\n",
    "sourcedata = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data'\n",
    "logdata    = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\supp\\log'\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    data_path = os.path.join(session_path)\n",
    "    try:\n",
    "        datapicker.reset(path=data_path)\n",
    "        create_dataset(datapicker=datapicker)\n",
    "        status = 1  # Success\n",
    "    except Exception as e:\n",
    "        status = 0  # Failed\n",
    "    \n",
    "    radiant_temp_status = 0  # Default to 0 if not found or if all zeros\n",
    "\n",
    "    if status == 1:\n",
    "        # Generate the sessions.tsv file\n",
    "        geodata = datapicker.geodata  # Retrieve geodata\n",
    "        create_geodata(geodata)\n",
    "\n",
    "        # Get GPS coordinates and integrate them into geodata\n",
    "        coords = datapicker.geodata.geometry.get_coordinates(include_z=True)\n",
    "        # Optionally rename the coordinate columns\n",
    "        coords.rename(columns={'y': 'latitude', 'x': 'longitude', 'z': 'elevation'}, inplace=True)\n",
    "        geodata = geodata.join(coords).drop(columns=['geometry'])\n",
    "\n",
    "        # Check if the radiant temperature column exists and contains non-zero values\n",
    "        if 'tk_thermocouple_temperature_value' in geodata:\n",
    "            if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "                radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "        # Save the full geodata to an Excel file in the log directory\n",
    "        log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "        \n",
    "        # Convert geodata to DataFrame if it's not already and save as Excel\n",
    "        if not isinstance(geodata, pd.DataFrame):\n",
    "            geodata = pd.DataFrame(geodata)\n",
    "        geodata.to_excel(geodata_file, index=True)\n",
    "    \n",
    "    # Log the result\n",
    "    ws.append([participant_name, session_name, status, radiant_temp_status])\n",
    "\n",
    "def create_geodata(geodata):\n",
    "    \"\"\"Compute UTCI across whole time series.\"\"\"\n",
    "    \n",
    "    # Define custom parameters\n",
    "    humidity = geodata['tk_humidity_humidity_value'] / 100  # in fraction\n",
    "    wind_speed = np.sqrt(geodata['atmos_northwind_value']**2 + geodata['atmos_eastwind_value']**2)  # m/s (~2.5 m of elevation)\n",
    "    temp_atmos = geodata['atmos_airtemperature_value']  # in ºC\n",
    "    temp_tk = geodata['tk_airquality_temperature_value'] / 100  # in ºC\n",
    "    temp_tk_ptc = geodata['tk_ptc_airtemp_value'] / 100  # in ºC\n",
    "    temp_radiant = geodata['tk_thermocouple_temperature_value'] / 100  # in ºC\n",
    "\n",
    "    # Assign custom parameters to the geodata attribute\n",
    "    geodata['humidity'] = humidity\n",
    "    geodata['wind_speed'] = wind_speed\n",
    "    geodata['temp_atmos'] = temp_atmos\n",
    "    geodata['temp_tk'] = temp_tk\n",
    "    geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "    geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "    # Compute the UTCI\n",
    "    geodata['utci'] = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "processed_participants = 0\n",
    "\n",
    "for participant_folder in os.listdir(sourcedata):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(sourcedata, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                process_session(session_path, participant_folder, session_folder)\n",
    "\n",
    "        processed_participants += 1\n",
    "        if processed_participants >= 43:\n",
    "            break  # Stop after processing 2 participants\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "wb.save(result_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\", \"Radiant Temperature\"])\n",
    "\n",
    "# Path information\n",
    "sourcedata = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data'\n",
    "logdata    = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\supp\\log'\n",
    "\n",
    "# To store the mean values for each session\n",
    "all_means = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    global all_means  # Declare all_means as global\n",
    "    data_path = os.path.join(session_path)\n",
    "    try:\n",
    "        datapicker.reset(path=data_path)\n",
    "        create_dataset(datapicker=datapicker)\n",
    "        status = 1  # Success\n",
    "    except Exception as e:\n",
    "        status = 0  # Failed\n",
    "    \n",
    "    radiant_temp_status = 0  # Default to 0 if not found or if all zeros\n",
    "\n",
    "    if status == 1:\n",
    "        # Generate the sessions.tsv file\n",
    "        geodata = datapicker.geodata  # Retrieve geodata\n",
    "        create_geodata(geodata)\n",
    "\n",
    "        # Get GPS coordinates and integrate them into geodata\n",
    "        coords = datapicker.geodata.geometry.get_coordinates(include_z=True)\n",
    "        coords.rename(columns={'y': 'latitude', 'x': 'longitude', 'z': 'elevation'}, inplace=True)\n",
    "        geodata = geodata.join(coords).drop(columns=['geometry'])\n",
    "\n",
    "        # Check if the radiant temperature column exists and contains non-zero values\n",
    "        if 'tk_thermocouple_temperature_value' in geodata:\n",
    "            if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "                radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "        # Save the full geodata to an Excel file in the log directory\n",
    "        log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "        geodata.to_excel(geodata_file, index=False)\n",
    "\n",
    "        # Compute the mean of all numerical columns\n",
    "        numeric_means = geodata.mean(numeric_only=True).to_dict()\n",
    "        numeric_means['Participant'] = participant_name\n",
    "        numeric_means['Session'] = session_name\n",
    "        all_means.append(numeric_means)  # Append to the global all_means list\n",
    "    \n",
    "    # Log the result\n",
    "    ws.append([participant_name, session_name, status, radiant_temp_status])\n",
    "\n",
    "\n",
    "def create_geodata(geodata):\n",
    "    \"\"\"Compute UTCI across the whole time series.\"\"\"\n",
    "    \n",
    "    # Define custom parameters\n",
    "    humidity = geodata['tk_humidity_humidity_value'] / 100  # in fraction\n",
    "    wind_speed = np.sqrt(geodata['atmos_northwind_value']**2 + geodata['atmos_eastwind_value']**2)  # m/s (~2.5 m of elevation)\n",
    "    temp_atmos = geodata['atmos_airtemperature_value']  # in ºC\n",
    "    temp_tk = geodata['tk_airquality_temperature_value'] / 100  # in ºC\n",
    "    temp_tk_ptc = geodata['tk_ptc_airtemp_value'] / 100  # in ºC\n",
    "    temp_radiant = geodata['tk_thermocouple_temperature_value'] / 100  # in ºC\n",
    "\n",
    "    # Assign custom parameters to the geodata attribute\n",
    "    geodata['humidity'] = humidity\n",
    "    geodata['wind_speed'] = wind_speed\n",
    "    geodata['temp_atmos'] = temp_atmos\n",
    "    geodata['temp_tk'] = temp_tk\n",
    "    geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "    geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "    # Compute the UTCI\n",
    "    geodata['utci'] = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "processed_participants = 0\n",
    "\n",
    "for participant_folder in os.listdir(sourcedata):\n",
    "    if participant_folder.startswith(\"OE\"):\n",
    "        participant_path = os.path.join(sourcedata, participant_folder)\n",
    "        for session_folder in os.listdir(participant_path):\n",
    "            session_path = os.path.join(participant_path, session_folder)\n",
    "            if os.path.isdir(session_path):\n",
    "                process_session(session_path, participant_folder, session_folder)\n",
    "\n",
    "        processed_participants += 1\n",
    "        if processed_participants >= 2:\n",
    "            break  # Stop after processing 2 participants\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "wb.save(result_file)\n",
    "\n",
    "# Save the mean values to a final CSV file\n",
    "if all_means:\n",
    "    final_means_df = pd.DataFrame(all_means)\n",
    "    final_means_file = os.path.join(logdata, \"final_means.csv\")\n",
    "    final_means_df.to_csv(final_means_file, index=False)\n",
    "    print(f\"Final CSV file with mean values saved to: {final_means_file}\")\n",
    "else:\n",
    "    print(\"No valid data found to compute mean values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore Dataset Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapicker.geodata\n",
    "\"\"\"Assess datapicker geodata information\n",
    "\n",
    "\"\"\"\n",
    "import pprint\n",
    "\n",
    "geo_data = datapicker.geodata.columns\n",
    "pprint.pprint(geo_data)\n",
    "\n",
    "streams_data = datapicker.dataset.streams\n",
    "pprint.pprint(dir(streams_data))\n",
    "\n",
    "tk_data = datapicker.dataset.streams.PupilLabs.Counter.Gaze\n",
    "pprint.pprint(dir(tk_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Create new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute UTCI across whole time series\n",
    "- pip install pythermalcomfort\n",
    "- See docs (https://pypi.org/project/pythermalcomfort/)\n",
    "UTCI:\n",
    "The parameters that are taken into account for calculating UTCI involve dry bulb temperature, mean radiation temperature, the pressure of water vapor or relative humidity, and wind speed (at the elevation of 10 m above the ground).\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pythermalcomfort.models import utci\n",
    "\n",
    "# Define custom parameters\n",
    "humidity = datapicker.geodata.tk_humidity_humidity_value / 100 # in % or fraction?\n",
    "wind_speed = np.sqrt(datapicker.geodata.atmos_northwind_value**2 + datapicker.geodata.atmos_eastwind_value**2) # m/s (~2.5 m of elevation)\n",
    "temp_atmos= datapicker.geodata.atmos_airtemperature_value # in ºC\n",
    "temp_tk = datapicker.geodata.tk_airquality_temperature_value/100 # in ºC)\n",
    "temp_tk_ptc = datapicker.geodata.tk_ptc_airtemp_value/100 # positive temperature coefficient (in ºC\n",
    "temp_radiant = datapicker.geodata.tk_thermocouple_temperature_value/100 # in ºC\n",
    "\n",
    "# Assign custom parameters to the geodata attribute\n",
    "datapicker.geodata['humidity'] = humidity\n",
    "datapicker.geodata['wind_speed'] = wind_speed\n",
    "datapicker.geodata['temp_atmos'] = temp_atmos\n",
    "datapicker.geodata['temp_tk'] = temp_tk\n",
    "datapicker.geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "datapicker.geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "# Compute the UTCI\n",
    "utci = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "datapicker.geodata['utci'] = utci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"interpret variables\n",
    "- utci\n",
    "    - See [here](Explicacao utci https://climate-adapt.eea.europa.eu/en/metadata/indicators/thermal-comfort-indices-universal-thermal-climate-index-1979-2019)\n",
    "\"\"\"\n",
    "exploremap(datapicker.geodata, column = 'tk_gps_data_value', cmap = 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploremap(geodata, column = 'geometry', cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Get geojson file for maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = '03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def find_file(number):\n",
    "    root = \"G:\\\\Shared drives\\\\Shared_FMUL_eMC\\\\WP5\\\\4_OutdoorExperiment\\\\LisbonWalks\\\\Useful Documents\\\\exp4percursos\\\\geojson_files\"\n",
    "\n",
    "    if number == '01':\n",
    "        filename = [file for file in os.listdir(root) if 'belem' in file][0]\n",
    "    elif number == '02':\n",
    "        filename = [file for file in os.listdir(root) if 'lapa' in file][0]\n",
    "    elif number == '03':\n",
    "        filename = [file for file in os.listdir(root) if 'anovas' in file][0]\n",
    "    elif number == '04':\n",
    "        filename = [file for file in os.listdir(root) if 'baixa' in file][0]\n",
    "    elif number == '05':\n",
    "        filename = [file for file in os.listdir(root) if 'graca' in file][0]\n",
    "    elif number == '06':\n",
    "        filename = [file for file in os.listdir(root) if 'pnacoes' in file][0]\n",
    "    else:\n",
    "        return \"Invalid number\"\n",
    "\n",
    "    filepath = os.path.join(root, filename)\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        return filepath\n",
    "    else:\n",
    "        return \"File not found\"\n",
    "\n",
    "print(find_file(str(numbers)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ideal path (customize for path)\n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "gdf = gpd.read_file(find_file(str(numbers)))\n",
    "\n",
    "# Calculate the mean latitude and longitude\n",
    "mean_lat = np.mean(gdf.geometry[0].coords.xy[1])\n",
    "mean_lon = np.mean(gdf.geometry[0].coords.xy[0])\n",
    "\n",
    "# Create a map centered at the mean of the coordinates in the LineString\n",
    "m = folium.Map(location=[mean_lat, mean_lon], zoom_start=13)\n",
    "\n",
    "# Add the LineString to the map\n",
    "folium.GeoJson(gdf.geometry[0]).add_to(m)\n",
    "\n",
    "# Show the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Visualize maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"exploremap kwargs\n",
    "- see (here)[https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html]\n",
    "- common cmaps:\n",
    "    - 'viridis'\n",
    "    - 'plasma'\n",
    "    - 'inferno'\n",
    "    - 'magma'\n",
    "    - 'cividis'\n",
    "    - 'hot'\n",
    "    - 'cool'\n",
    "    - 'spring'\n",
    "\"\"\"\n",
    "\n",
    "map = exploremap(datapicker.geodata, cmap = 'spring')\n",
    "folium.GeoJson(gdf.geometry[0]).add_to(map)\n",
    "map # Display the map (can be exported to html if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore Dataset Streams (without EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revts = datapicker.dataset.streams.EEG.server_lsl_marker\n",
    "evts = revts[revts.MarkerIdx>35000]\n",
    "print(evts)# Type -> <class 'pandas.core.frame.DataFrame'>\n",
    "if len(evts[evts.MarkerIdx<35099])>4: # Check if acquisition has baseline\n",
    "    baseline=True\n",
    "else:\n",
    "    baseline=False\n",
    "print(baseline)\n",
    "# ---------------------------------------------------------------\n",
    "revts = datapicker.dataset.streams.EEG.server_lsl_marker\n",
    "evts = revts[revts.MarkerIdx>35000]\n",
    "if baseline:\n",
    "    # Automatically pass checkpoints to the plot_traces function\n",
    "    beg_bs_close = evts[evts.MarkerIdx==35001].index[0]\n",
    "    end_bs_close = evts[evts.MarkerIdx==35002].index[0]\n",
    "    beg_bs_open = evts[evts.MarkerIdx==35003].index[0]\n",
    "    end_bs_open = evts[evts.MarkerIdx==35004].index[0]\n",
    "    beg = evts[evts.MarkerIdx==35005].index[0]\n",
    "    end = evts[evts.MarkerIdx==35006].index[0]\n",
    "    beg_bs_close_seconds = evts.loc[beg_bs_close, 'Seconds']\n",
    "    end_bs_close_seconds = evts.loc[end_bs_close, 'Seconds']\n",
    "    beg_bs_open_seconds = evts.loc[beg_bs_open, 'Seconds']\n",
    "    end_bs_open_seconds = evts.loc[end_bs_open, 'Seconds']\n",
    "    beg_seconds = evts.loc[beg, 'Seconds']\n",
    "    end_seconds = evts.loc[end, 'Seconds']\n",
    "\n",
    "    evts = evts[evts.MarkerIdx>35099] # Remove the markers ids < 35099\n",
    "    indexes = evts[(evts.MarkerIdx //100) % 10 == 1].index # Get the indexes of the markers\n",
    "    sub_dfs = [evts.loc[indexes[i]:indexes[i+1]-1] for i in range(len(indexes)-1)] # Divide the DataFrame by the indexes\n",
    "    sub_dfs.append(evts.loc[indexes[-1]:])  # Add the last segment from the last index to the end of the DataFrame\n",
    "\n",
    "    chk_segments = []\n",
    "    list_dict = {1: [1, 0.1, 0.1], 2: [0.1, 1, 0.1], 3: [0.1, 0.1, 0.1]} # Red, Green, Black\n",
    "\n",
    "    for df in sub_dfs:\n",
    "        for i in range(1, 4):\n",
    "            index = df[(df.MarkerIdx // 100) % 10 == i].first_valid_index()\n",
    "            if index is not None:\n",
    "                chk_segments.append((df.loc[index, 'Seconds'], list_dict[i]))\n",
    "\n",
    "    # Prepend beg and append end to chk_segments\n",
    "    chk_segments.insert(0, (beg_bs_close_seconds, [128/255, 0, 128/255])) # Purple color to eyes closed baseline\n",
    "    chk_segments.insert(1, (end_bs_close_seconds, [1, 1, 1])) # White color to end\n",
    "    chk_segments.insert(2, (beg_bs_open_seconds, [0.1, 0.1, 1])) # Blue color to eyes open baseline\n",
    "    chk_segments.insert(3, (end_bs_open_seconds, [1, 1, 1])) # White color to end\n",
    "    chk_segments.insert(4, (beg_seconds, [0.1, 0.1, 0.1])) # Black color to beginning\n",
    "    chk_segments.append((end_seconds, [0.1, 0.1, 0.1])) # Black color to end\n",
    "    \n",
    "else:\n",
    "    first = evts.loc[evts.index[0], 'Seconds']\n",
    "    beg = evts[evts.MarkerIdx==35001].index[0]\n",
    "    end = evts[evts.MarkerIdx==35002].index[0] # Will not work when double-clicked\n",
    "    beg_seconds = evts.loc[beg, 'Seconds']\n",
    "    end_seconds = evts.loc[end, 'Seconds']\n",
    "\n",
    "    evts = evts[evts.MarkerIdx>35099] # Remove the markers ids < 35099\n",
    "    indexes = evts[(evts.MarkerIdx //100) % 10 == 1].index # Get the indexes of the markers\n",
    "    sub_dfs = [evts.loc[indexes[i]:indexes[i+1]-1] for i in range(len(indexes)-1)] # Divide the DataFrame by the indexes\n",
    "    sub_dfs.append(evts.loc[indexes[-1]:])  # Add the last segment from the last index to the end of the DataFrame\n",
    "\n",
    "    chk_segments = []\n",
    "    list_dict = {1: [1, 0.1, 0.1], 2: [0.1, 1, 0.1], 3: [0.1, 0.1, 0.1]} # Red, Green, Black\n",
    "\n",
    "    for df in sub_dfs:\n",
    "        for i in range(1, 4):\n",
    "            index = df[(df.MarkerIdx // 100) % 10 == i].first_valid_index()\n",
    "            if index is not None:\n",
    "                chk_segments.append((df.loc[index, 'Seconds'], list_dict[i]))\n",
    "\n",
    "    chk_segments.insert(0, (first, [1, 1, 1])) # Black color to beginning\n",
    "    chk_segments.insert(1, (beg_seconds, [0.1, 0.1, 0.1])) # Black color to beginning\n",
    "    chk_segments.append((end_seconds, [1, 1, 1])) # Black color to end\n",
    "\n",
    "print(chk_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Plot Atmospheric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces({\n",
    "    'T': datapicker.dataset.streams.TK.AirQuality.Temperature.data/100,\n",
    "    'T_PTC': datapicker.dataset.streams.TK.PTC.AirTemp.data/100,\n",
    "    'SL(mA)': datapicker.dataset.streams.TK.Dual0_20mA.SolarLight.data/1000000,\n",
    "    'Humi(%)':datapicker.dataset.streams.TK.Humidity.Humidity.data/100,\n",
    "    }, \n",
    "    segments = chk_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Plot PMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces({\n",
    "    'pm10': datapicker.dataset.streams.TK.ParticulateMatter.PM10_0.data,\n",
    "    'pm2.5':datapicker.dataset.streams.TK.ParticulateMatter.PM2_5.data,\n",
    "    'pm1':datapicker.dataset.streams.TK.ParticulateMatter.PM1_0.data,\n",
    "    }, \n",
    "    segments = chk_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Plot Empatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapicker.dataset.streams.Empatica.data.Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces({\n",
    "    'GSR': datapicker.dataset.streams.Empatica.data.E4_Gsr['Value'],\n",
    "    'HR': datapicker.dataset.streams.Empatica.data.E4_Hr['Value'],\n",
    "    'BVP': datapicker.dataset.streams.Empatica.data.E4_Bvp['Value'],\n",
    "    'IBI': datapicker.dataset.streams.Empatica.data.E4_Ibi['Value'],\n",
    "    'T': datapicker.dataset.streams.Empatica.data.E4_Temperature['Value'],\n",
    "    'Battery': datapicker.dataset.streams.Empatica.data.E4_Battery['Value'],\n",
    "    }, \n",
    "    segments = chk_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapicker.dataset.streams.PupilLabs.Counter.Gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use this cell to obtain other types of information from the datapicker object.\n",
    "For example, the following code will display the type of the geodata attribute.\n",
    "Use it to export some information.\n",
    "\"\"\"\n",
    "# Save as geojson file (without separated coordinates)\n",
    "# datapicker.geodata.to_file(r'C:\\git\\JoaoAmaro2001\\notebooks\\src\\output\\geodata.geojson', driver='GeoJSON') \n",
    "\n",
    "# Get GPS coordinates\n",
    "coords = datapicker.geodata.geometry.get_coordinates(include_z=True)\n",
    "# coords.rename(columns = {'y':'latitude', 'x':'longitude','z':'elevation'}, inplace = True) # rename if needed\n",
    "dataexport = datapicker.geodata.join(coords).drop(columns=['geometry'])\n",
    "\n",
    "# datapicker.geodata <class 'geopandas.geodataframe.GeoDataFrame'>\n",
    "dataexport.to_csv(r'C:\\Users\\joaop\\git\\JoaoAmaro2001\\notebooks\\src\\output\\geodata.csv') # Save the data to a csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = datapicker.dataset.streams.EEG\n",
    "\"\"\" Additional information about the dataset\n",
    "\"\"\"\n",
    "print(dir(datapicker.dataset.streams.EEG))\n",
    "print(dir(datapicker.dataset.streams.EEG.data.np_markers)) # See attributes of the data\n",
    "print(datapicker.dataset.streams.EEG_dir__) # See type of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg.data.np_eeg.shape) # Get the shape (dims) of the EEG data\n",
    "print(eeg.data._NedfReader__get_info()) # Get the information of the EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revts = datapicker.dataset.streams.EEG.server_lsl_marker\n",
    "evts = revts[revts.MarkerIdx>35000]\n",
    "print(evts)# Type -> <class 'pandas.core.frame.DataFrame'>\n",
    "if len(evts[evts.MarkerIdx<35099])>4: # Check if acquisition has baseline\n",
    "    baseline=True\n",
    "else:\n",
    "    baseline=False\n",
    "print(baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Plot with EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces({\n",
    "    'eeg': datapicker.dataset.streams.EEG.data,    \n",
    "    'ecg': datapicker.dataset.streams.BioData.ECG.data,\n",
    "    'altitude': datapicker.dataset.georeference.elevation,\n",
    "    }, \n",
    "    segments = chk_segments\n",
    "    # figsize = (16,8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "eeg.data.np_eeg[14622, 1]\n",
    "plt.plot(eeg.data.np_eeg)  # np_eeg is a numpy array with the EEG data - for processing purposes\n",
    "plt.xlim(evts['EegSample'].iloc[0], evts['EegSample'].iloc[-1])  # Set x-axis limits (beginning and end of the event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Export EEG to EEGLAB (.set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Required packages\n",
    "pip install eeglabio\n",
    "pip install mne\n",
    "\"\"\"\n",
    "import mne\n",
    "from eeglabio.utils import export_mne_raw\n",
    "raw = mne.io.read_raw(...)\n",
    "export_mne_raw(raw, \"file_name.set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos\n",
    "help(mne.io.read_raw_nedf) # important to use this function?\n",
    "import inspect\n",
    "print(inspect.getsource(mne.io.read_raw_nedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "datafolder = datapicker._pathlist.value\n",
    "nedf_files = glob.glob(os.path.join(datafolder, '*.nedf'))\n",
    "if nedf_files:\n",
    "    filename = nedf_files[0]\n",
    "else:\n",
    "    print(\"No .nedf files found in the directory.\")\n",
    "print(filename)\n",
    "eeg_nedf = mne.io.read_raw_nedf(filename, preload=False, verbose=None)\n",
    "print(type(eeg_nedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(eeg_nedf))\n",
    "print(dir(eeg_nedf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export Dataset to OGC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = DatasetRecord(datapicker.dataset, datapicker.geodata, properties=RecordProperties(\n",
    "    title='<City> Outdoor Walk: <CityRegion> Subject <ID>',\n",
    "    description='Outdoor walk data collection',\n",
    "    license='CC BY-NC 4.0',\n",
    "    tool='Bonsai',\n",
    "    keywords=['<City>', 'Outdoor', 'Walk', 'Microclimate', 'Biosignals'],\n",
    "    contacts=[\n",
    "        Contact(\n",
    "            name='Your Name',\n",
    "            institution='Your Institution',\n",
    "            email='youremail@yourdomain.com'\n",
    "        )\n",
    "    ],\n",
    "    themes=[]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = Path(record.id)\n",
    "export_geoframe_to_geojson(datapicker.geodata, rpath.with_suffix('.geojson'))\n",
    "with open(rpath.with_suffix('.json'), 'w') as f:\n",
    "    f.write(record.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quesions for NGR\n",
    "\n",
    "- [x] How can I make plots interactable? I want to zoom in and out. Furthermore, I want to check different timepoints.\n",
    "- [x] Meaning of EEG events?\n",
    "- [ ] Can I overlap maps on the openstreetmap plot? Such as a .kmz file.\n",
    "- [x] How can I see pupilabs eye-tracking data?\n",
    "- [ ] How to export to SDI? Should I do it?\n",
    "- [x] Why is empatica_hr black in most places?\n",
    "- [ ] Can `eeg.py` use MNE's nedf import function? It would easily allow exporting data to matlab.\n",
    "- [ ] Can we change the base map? OpenStreetMaps has different views.\n",
    "- [ ] Will you update the datapicker.geodata columns (e.g. skin surface temperature)?  \n",
    "- [ ] What warning is this?\n",
    "```python\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "```\n",
    "- [x] Why can I not load `I:\\João\\Exp_4-outdoor_experiment\\OE003\\Lisbon_Baixa_sub-OE104003_2024-04-16T160350Z\\`? TIM.csv problem.\n",
    "```python\n",
    "{\n",
    "\t\"name\": \"KeyError\",\n",
    "\t\"message\": \"'Class'\",\n",
    "\t\"stack\": \"---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\ipyfilechooser\\\\filechooser.py:317, in FileChooser._on_select_click(self, _b)\n",
    "    315 if self._callback is not None:\n",
    "    316     try:\n",
    "--> 317         self._callback(self)\n",
    "    318     except TypeError:\n",
    "    319         # Support previous behaviour of not passing self\n",
    "    320         self._callback()\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\src\\\\ingestion\\\\modules.py:43, in create_datapicker.<locals>.dataset_changed(chooser)\n",
    "     41 display(chooser)\n",
    "     42 print(f\\\"Loading dataset: {Path(chooser.selected_path).name}...\\\" )\n",
    "---> 43 dataset = load_dataset(chooser.selected_path, schema=custom_schema)\n",
    "     44 print(f\\\"Dataset: {dataset} loaded successfully, and {'not' if not dataset.has_calibration else 'sucessfully'} calibrated.\\\" )\n",
    "     45 plot_summary(dataset)\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\src\\\\ingestion\\\\helpers.py:27, in load_dataset(root, schema, reload, export_path)\n",
    "     22 dataset.populate_streams(autoload=False)  # Add the \\\"schema\\\" that we want to load to our Dataset. If we want to load the whole dataset automatically, set autoload to True.\n",
    "     24 if reload:\n",
    "     25     # We will just load every single stream at the same time. This might take a while if loading from AWS\n",
    "     26     # Some warnings will be printed if some sensors were not acquired during the experiment. These are normal and can be usually ignored.\n",
    "---> 27     dataset.reload_streams(force_load=True)\n",
    "     28     sync_lookup = dataset.calibrate_ubx_to_harp()\n",
    "     29     dataset.add_ubx_georeference()\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\schema\\\\__init__.py:113, in Dataset.reload_streams(self, force_load)\n",
    "    111 for stream in self._iter_schema_streams(self.streams):\n",
    "    112     if force_load is True:\n",
    "--> 113         stream.load()\n",
    "    114     else:\n",
    "    115         if stream.autoload is True:\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\stream\\\\ubx.py:29, in UbxStream.load(self)\n",
    "     28 def load(self):\n",
    "---> 29 \\tself.load_event_list(self.autoload_messages)\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\stream\\\\ubx.py:46, in UbxStream.load_event_list(self, events)\n",
    "     44 def load_event_list(self, events: list):\n",
    "     45 \\tfor event in events:\n",
    "---> 46 \\t\\tself.load_event(event)\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\stream\\\\ubx.py:33, in UbxStream.load_event(self, event)\n",
    "     31 \\tdef load_event(self, event: _UBX_MSGIDS):\n",
    "     32 \\t\\tself._update_dotmap(event,\n",
    "---> 33                       load_ubx_event_stream(\n",
    "     34                           event,\n",
    "     35                           root=self.rootfolder)\n",
    "     36                       )\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\io\\\\ubx.py:158, in load_ubx_event_stream(ubxmsgid, root, ubxfolder)\n",
    "    152 bin_file = load_ubx_bin_event(ubxmsgid=ubxmsgid,\n",
    "    153                               root=root,\n",
    "    154                               ubxfolder=ubxfolder)\n",
    "    155 csv_file = load_ubx_harp_ts_event(ubxmsgid=ubxmsgid,\n",
    "    156                                   root=root,\n",
    "    157                                   ubxfolder=ubxfolder)\n",
    "--> 158 if (bin_file['Class'].values == csv_file['Class'].values).all():\n",
    "    159     bin_file['Seconds'] = csv_file.index\n",
    "    160     bin_file = bin_file.set_index('Seconds')\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py:4090, in DataFrame.__getitem__(self, key)\n",
    "   4088 if self.columns.nlevels > 1:\n",
    "   4089     return self._getitem_multilevel(key)\n",
    "-> 4090 indexer = self.columns.get_loc(key)\n",
    "   4091 if is_integer(indexer):\n",
    "   4092     indexer = [indexer]\n",
    "\n",
    "File c:\\\\git\\\\JoaoAmaro2001\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\indexes\\\\range.py:417, in RangeIndex.get_loc(self, key)\n",
    "    415         raise KeyError(key) from err\n",
    "    416 if isinstance(key, Hashable):\n",
    "--> 417     raise KeyError(key)\n",
    "    418 self._check_indexing_error(key)\n",
    "    419 raise KeyError(key)\n",
    "\n",
    "KeyError: 'Class'\"\n",
    "}\n",
    "```\n",
    "- [ ] Why can I not load `Lisbon_Belem_sub-OE101005_2024-05-10T111823Z` (after gps synch fix)?\n",
    "```python\n",
    "Failed Stream Harp stream from device \t\tTK, stream GPS_Latitude(227): Input dataframe is empty.\n",
    "Failed Stream Harp stream from device \t\tTK, stream GPS_Longitude(228): Input dataframe is empty.\n",
    "Failed Stream Harp stream from device \t\tTK, stream GPS_Altitude(229): Input dataframe is empty.\n",
    "Failed Stream Harp stream from device \t\tTK, stream GPS_HasFix(232): Input dataframe is empty.\n",
    "```\n",
    "- [ ] Why do I get this output from loading utci into map? Not a geopandas? Is it important?\n",
    "```python\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\geopandas\\geodataframe.py:1645: DeprecationWarning: Passing a SingleBlockManager to Series is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  srs = pd.Series(*args, **kwargs)\n",
    "c:\\git\\JoaoAmaro2001\\notebooks\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:706: DeprecationWarning: Passing a BlockManager to GeoDataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
    "  warnings.warn(\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the plot_traces function (OE102009)\n",
    "plot_traces({\n",
    "    'T(C)': datapicker.dataset.streams.TK.AirQuality.Temperature.data/100,\n",
    "    'altitude': datapicker.dataset.georeference.elevation,\n",
    "    'ecg': datapicker.dataset.streams.BioData.ECG.data,\n",
    "    'eeg': datapicker.dataset.streams.EEG.data,\n",
    "    'gsr': datapicker.geodata.empatica_e4_gsr,\n",
    "    }, \n",
    "    segments=[\n",
    "    (evts['Seconds'][beg.item()], [0.1, 0.1, 0.1]), \n",
    "    (evts['Seconds'][5], [1, 0.1, 0.1]),\n",
    "    (evts['Seconds'][11], [0.1, 1, 0.1]),\n",
    "    (evts['Seconds'][14], [0.1, 0.1, 0.1]),\n",
    "    (evts['Seconds'][22], [1, 0.1, 0.1]),\n",
    "    (evts['Seconds'][28], [0.1, 1, 0.1]),\n",
    "    (evts['Seconds'][31], [0.1, 0.1, 0.1]),\n",
    "    (evts['Seconds'][92], [1, 0.1, 0.1]),\n",
    "    (evts['Seconds'][99], [0.1, 1, 0.1]),\n",
    "    (evts['Seconds'][102], [0.1, 0.1, 0.1])\n",
    "    ],\n",
    "    figsize = (40,10)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
