{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from modules import *\n",
    "from missing_sync import build_schema\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get directories for faulty datasets\n",
    "\n",
    "datasets without GPS (13 in total):\n",
    "OE101001\n",
    "OE101002\n",
    "OE102002\n",
    "OE102003\n",
    "OE104003\n",
    "OE104004\n",
    "OE101012\n",
    "OE106005\n",
    "OE106011\n",
    "OE106010\n",
    "OE106016\n",
    "OE106021\n",
    "OE106032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import utils.for_setpath as path\n",
    "\n",
    "# Define the list of participant codes without GPS\n",
    "participant_codes = [\n",
    "    'OE101001',\n",
    "    'OE101002',\n",
    "    'OE102002',\n",
    "    'OE102003',\n",
    "    'OE104003',\n",
    "    'OE104004',\n",
    "    'OE101012',\n",
    "    'OE106005',\n",
    "    'OE106011',\n",
    "    'OE106010',\n",
    "    'OE106016',\n",
    "    'OE106021',\n",
    "    'OE106032'\n",
    "]\n",
    "\n",
    "# Define the sessions mapping (session name to session number)\n",
    "sessions = [\n",
    "    ('Baixa', 4),\n",
    "    ('Belem', 1),\n",
    "    ('Parque', 6),\n",
    "    ('Gulbenkian', 3),\n",
    "    ('Lapa', 2),\n",
    "    ('Gra√ßa', 5)\n",
    "]\n",
    "session_name_to_num = {name: num for name, num in sessions}\n",
    "\n",
    "# Define the base directories\n",
    "sourcedata = path.sourcedata\n",
    "\n",
    "# Initialize a list to store participant information\n",
    "participants_info = [] \n",
    "\n",
    "# Loop over each participant code\n",
    "for participant_code in participant_codes:\n",
    "    try:\n",
    "        # Parse the participant code to get acquisition information\n",
    "        participant_info = parse_participant_code(participant_code)\n",
    "        city = participant_info['city']\n",
    "        subject_id = participant_info['subject_id']\n",
    "        session_name = participant_info['session']\n",
    "        stimulus = participant_info['stimulus']\n",
    "        \n",
    "        # Get the session number using the session name\n",
    "        ses_num = session_name_to_num.get(session_name)\n",
    "        if ses_num is None:\n",
    "            print(f\"Session name '{session_name}' not recognized for participant code '{participant_code}'.\")\n",
    "            continue  # Skip to the next participant code if session name is invalid\n",
    "        \n",
    "        # Format the subject_id and session number with leading zeros\n",
    "        subject_id_str = f\"{subject_id:03d}\"  # Format subject_id as a 3-digit number\n",
    "        \n",
    "        # Search for the session folder by name\n",
    "        subject_dir = os.path.join(sourcedata, 'data', f\"OE{subject_id_str}\")\n",
    "        if not os.path.exists(subject_dir):\n",
    "            print(f\"Subject directory not found for {participant_code}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        session_folder = None\n",
    "        for folder_name in os.listdir(subject_dir):\n",
    "            if session_name.lower() in folder_name.lower():\n",
    "                session_folder = os.path.join(subject_dir, folder_name)\n",
    "                break\n",
    "        \n",
    "        if session_folder is None:\n",
    "            print(f\"Session folder containing '{session_name}' not found for participant {participant_code}.\")\n",
    "            continue\n",
    "        \n",
    "        # Print the participant code and the corresponding session folder path\n",
    "        print(f\"Participant Code: {participant_code}\")\n",
    "        print(f\"Subject ID: {subject_id}\")\n",
    "        print(f\"Session Name: {session_name}\")\n",
    "        print(f\"Session Folder: {session_folder}\")\n",
    "        print(\"----------------------------\")\n",
    "        \n",
    "        # Store participant information in a dictionary\n",
    "        participant_data = {\n",
    "            'participant_code': participant_code,\n",
    "            'city': city,\n",
    "            'subject_id': subject_id,\n",
    "            'session_name': session_name,\n",
    "            'session_number': ses_num,\n",
    "            'session_folder': session_folder\n",
    "        }\n",
    "        participants_info.append(participant_data)\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(f\"Error parsing participant code '{participant_code}': {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for participant code '{participant_code}': {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import utils.for_setpath as path\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Participant Name\", \"Session Name\", \"Status\", \"Radiant Temperature\"])\n",
    "\n",
    "# Path information\n",
    "sourcedata = os.path.join(path.sourcedata, 'data')\n",
    "logdata    = os.path.join(path.sourcedata, 'supp','log')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "def process_session(session_path, participant_name, session_name):\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        data_path  = session_path\n",
    "        datapicker = create_datapicker(path=data_path, schema=build_schema, calibrate_ubx_to_harp=False)\n",
    "        dataset    = load_dataset(datapicker.selected_path, ubx=True, unity=False, calibrate_ubx_to_harp=False, schema=build_schema)\n",
    "        status = 1  # Success\n",
    "\n",
    "        # Create geodata\n",
    "        geodata = dataset.to_geoframe()\n",
    "\n",
    "        # Process geodata\n",
    "        geodata['time'] = geodata.index.to_pydatetime()\n",
    "        geodata         = tidy_geodata(geodata)\n",
    "        geodata         = add_environmental_metrics(geodata)\n",
    "        geodata         = correct_gps_data(geodata)\n",
    "\n",
    "        # Check if the radiant temperature column exists and contains non-zero values\n",
    "        radiant_temp_status = 0  # Default to 0\n",
    "        if 'tk_thermocouple_temperature_value' in geodata.columns:\n",
    "            if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "                radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "        # Save the full geodata to an Excel file in the log directory\n",
    "        log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "\n",
    "        # Ensure geodata is a DataFrame and save as Excel\n",
    "        if not isinstance(geodata, pd.DataFrame):\n",
    "            geodata = pd.DataFrame(geodata)\n",
    "        geodata.to_excel(geodata_file, index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for participant '{participant_name}', session '{session_name}': {e}\")\n",
    "        print(\"Most likely needs to have an updated EEG .nedf file\")\n",
    "        status = 0  # Failed\n",
    "        radiant_temp_status = 0  # Default to 0\n",
    "\n",
    "    # Log the result in the Excel sheet\n",
    "    ws.append([participant_name, session_name, status, radiant_temp_status])\n",
    "\n",
    "# MAIN SCRIPT\n",
    "for folder in participants_info:\n",
    "    subj_folder = folder['session_folder']\n",
    "    subj_name = f\"OE{folder['subject_id']:03d}\"\n",
    "    session_name = folder['session_name']\n",
    "    process_session(subj_folder, subj_name, session_name)\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "wb.save(result_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "from pluma.schema.outdoor import build_schema\n",
    "%matplotlib widget\n",
    "datapicker = create_datapicker(path=r'',schema=build_schema, calibrate_ubx_to_harp=False)\n",
    "display(datapicker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapicker = create_datapicker(path=r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\data\\OE003', schema=build_schema)\n",
    "display(datapicker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore Dataset Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exploremap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexploremap\u001b[49m(dataset\u001b[38;5;241m.\u001b[39mto_geoframe(), column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtk_thermocouple_temperature_value\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exploremap' is not defined"
     ]
    }
   ],
   "source": [
    "exploremap(dataset.to_geoframe(), column='tk_thermocouple_temperature_value', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore Dataset Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dataset.streams.EEG.data)\n",
    "# print(datapicker.dataset.streams.EEG.data)\n",
    "\n",
    "# datapicker.dataset.streams.EEG.data[\"np_time\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG = datapicker.dataset.streams.EEG.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG.np_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapicker.dataset.streams.EEG.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces({\n",
    "    'accelX': datapicker.dataset.streams.Accelerometer.data[\"Accl_X\"],\n",
    "    'temp (C)': datapicker.dataset.streams.TK.AirQuality.Temperature.data/100,\n",
    "    'angleX': datapicker.dataset.streams.Accelerometer.data[\"Orientation_X\"],\n",
    "    'altitude': datapicker.dataset.georeference.elevation,\n",
    "    'iaq': datapicker.dataset.streams.TK.AirQuality.IAQIndex.data,\n",
    "    'eeg': datapicker.dataset.streams.EEG.data.np_eeg,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to IGOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from pythermalcomfort.models import utci\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Initialize the Excel workbook and sheet\n",
    "\n",
    "\n",
    "# Path information\n",
    "#logdata    = r'Z:\\Exp_4-outdoor_walk\\lisbon\\sourcedata\\supp\\log'\n",
    "\n",
    "# To store the mean values for each session\n",
    "# all_means = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "\n",
    "radiant_temp_status = 0  # Default to 0 if not found or if all zeros\n",
    "\n",
    "# Generate the sessions.tsv file\n",
    "geodata = datapicker.geodata  # Retrieve geodata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define custom parameters\n",
    "humidity = geodata['tk_humidity_humidity_value'] / 100  # in fraction\n",
    "wind_speed = np.sqrt(geodata['atmos_northwind_value']**2 + geodata['atmos_eastwind_value']**2)  # m/s (~2.5 m of elevation)\n",
    "temp_atmos = geodata['atmos_airtemperature_value']  # in ¬∫C\n",
    "temp_tk = geodata['tk_airquality_temperature_value'] / 100  # in ¬∫C\n",
    "temp_tk_ptc = geodata['tk_ptc_airtemp_value'] / 100  # in ¬∫C\n",
    "temp_radiant = geodata['tk_thermocouple_temperature_value'] / 100  # in ¬∫C\n",
    "\n",
    "# Assign custom parameters to the geodata attribute\n",
    "geodata['humidity'] = humidity\n",
    "geodata['wind_speed'] = wind_speed\n",
    "geodata['temp_atmos'] = temp_atmos\n",
    "geodata['temp_tk'] = temp_tk\n",
    "geodata['temp_tk_ptc'] = temp_tk_ptc\n",
    "geodata['temp_radiant'] = temp_radiant\n",
    "\n",
    "# Compute the UTCI\n",
    "geodata['utci'] = utci(tdb=temp_atmos, tr=temp_radiant, v=wind_speed, rh=humidity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get GPS coordinates and integrate them into geodata\n",
    "coords = datapicker.geodata.geometry.get_coordinates(include_z=True)\n",
    "coords.rename(columns={'y': 'latitude', 'x': 'longitude', 'z': 'elevation'}, inplace=True)\n",
    "geodata = geodata.join(coords).drop(columns=['geometry'])\n",
    "\n",
    "# Check if the radiant temperature column exists and contains non-zero values\n",
    "if 'tk_thermocouple_temperature_value' in geodata:\n",
    "    if geodata['tk_thermocouple_temperature_value'].sum() != 0:\n",
    "        radiant_temp_status = 1  # Non-zero values exist\n",
    "\n",
    "# Save the full geodata to an Excel file in the log directory\n",
    "#log_folder = os.path.join(logdata, f\"sub-{participant_name}\", f\"ses-{session_name}\")\n",
    "#os.makedirs(log_folder, exist_ok=True)\n",
    "# geodata_file = os.path.join(log_folder, f\"sub-{participant_name}_ses-{session_name}_geodata.xlsx\")\n",
    "# geodata.to_excel(geodata_file, index=False)\n",
    "\n",
    "# Compute the mean of all numerical columns\n",
    "# numeric_means = geodata.mean(numeric_only=True).to_dict()\n",
    "# numeric_means['Participant'] = participant_name\n",
    "# numeric_means['Session'] = session_name\n",
    "# all_means.append(numeric_means)  # Append to the global all_means list\n",
    "\n",
    "# Log the result\n",
    "\n",
    "\n",
    "# def create_geodata(geodata):\n",
    "#    \"\"\"Compute UTCI across the whole time series.\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "# processed_participants = 0\n",
    "\n",
    "# for participant_folder in os.listdir(sourcedata):\n",
    "#     if participant_folder.startswith(\"OE\"):\n",
    "#         participant_path = os.path.join(sourcedata, participant_folder)\n",
    "#         for session_folder in os.listdir(participant_path):\n",
    "#             session_path = os.path.join(participant_path, session_folder)\n",
    "#             if os.path.isdir(session_path):\n",
    "#process_session( participant_folder, session_folder)\n",
    "\n",
    "        # processed_participants += 1\n",
    "        # if processed_participants >= 2:\n",
    "        #     break  # Stop after processing 2 participants\n",
    "\n",
    "# Save the Excel file with the updated status in the log directory\n",
    "# result_file = os.path.join(logdata, \"session_processing_results.xlsx\")\n",
    "# wb.save(result_file)\n",
    "\n",
    "# Save the mean values to a final CSV file\n",
    "# if all_means:\n",
    "#     final_means_df = pd.DataFrame(all_means)\n",
    "#     final_means_file = os.path.join(logdata, \"final_means.csv\")\n",
    "#     final_means_df.to_csv(final_means_file, index=False)\n",
    "#     print(f\"Final CSV file with mean values saved to: {final_means_file}\")\n",
    "# else:\n",
    "#     print(\"No valid data found to compute mean values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapicker.geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = datapicker.selected_path.split('\\\\')\n",
    "strs[len(strs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata = r'c:\\EXPORT'\n",
    "final_means_file = os.path.join(logdata, strs[len(strs)-1]+'.csv')\n",
    "datapicker.geodata.to_csv(final_means_file, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export Dataset to OGC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = DatasetRecord(datapicker.dataset, datapicker.geodata, properties=RecordProperties(\n",
    "    title='<City> Outdoor Walk: <CityRegion> Subject <ID>',\n",
    "    description='Outdoor walk data collection',\n",
    "    license='CC BY-NC 4.0',\n",
    "    tool='Bonsai',\n",
    "    keywords=['<City>', 'Outdoor', 'Walk', 'Microclimate', 'Biosignals'],\n",
    "    contacts=[\n",
    "        Contact(\n",
    "            name='Your Name',\n",
    "            institution='Your Institution',\n",
    "            email='youremail@yourdomain.com'\n",
    "        )\n",
    "    ],\n",
    "    themes=[]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = Path(record.id)\n",
    "export_geoframe_to_geojson(datapicker.geodata, rpath.with_suffix('.geojson'))\n",
    "with open(rpath.with_suffix('.json'), 'w') as f:\n",
    "    f.write(record.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy version\n",
    "\n",
    "The output of the following was: `2.1.3`\n",
    "```python\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "```\n",
    "\n",
    "The error was:\n",
    "```python\n",
    "{\n",
    "\t\"name\": \"AttributeError\",\n",
    "\t\"message\": \"`np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.\",\n",
    "\t\"stack\": \"---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\ipyfilechooser\\\\filechooser.py:317, in FileChooser._on_select_click(self, _b)\n",
    "    315 if self._callback is not None:\n",
    "    316     try:\n",
    "--> 317         self._callback(self)\n",
    "    318     except TypeError:\n",
    "    319         # Support previous behaviour of not passing self\n",
    "    320         self._callback()\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\src\\\\ingestion\\\\modules.py:46, in create_datapicker.<locals>.dataset_changed(chooser)\n",
    "     44 display(chooser)\n",
    "     45 print(f\\\"Loading dataset: {Path(chooser.selected_path).name}...\\\" )\n",
    "---> 46 dataset = load_dataset(chooser.selected_path, ubx=ubx, unity=unity, calibrate_ubx_to_harp=calibrate_ubx_to_harp, schema=schema)\n",
    "     47 print(f\\\"Dataset: {dataset} loaded successfully, and {'not' if not dataset.has_calibration else 'sucessfully'} calibrated.\\\")\n",
    "     48 chooser.dataset = dataset\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\src\\\\ingestion\\\\helpers.py:22, in load_dataset(root, schema, reload, ubx, unity, calibrate_ubx_to_harp, export_path)\n",
    "     15 def load_dataset(root, schema, reload=True, ubx=True, unity=False, calibrate_ubx_to_harp=True, export_path=None):\n",
    "     16     # Path to the dataset. Can be local or remote.\n",
    "     17     dataset = Dataset(\n",
    "     18         root=root,\n",
    "     19         datasetlabel=\\\"FMUL_\\\" + root.split(\\\"\\\\\\\\\\\")[-1],\n",
    "     20         georeference= Georeference(),\n",
    "     21         schema=schema)  # Create a Dataset object that will contain the ingested data.\n",
    "---> 22     dataset.populate_streams(autoload=False)  # Add the \\\"schema\\\" that we want to load to our Dataset. If we want to load the whole dataset automatically, set autoload to True.\n",
    "     24     if reload:\n",
    "     25         # We will just load every single stream at the same time. This might take a while if loading from AWS\n",
    "     26         # Some warnings will be printed if some sensors were not acquired during the experiment. These are normal and can be usually ignored.\n",
    "     27         dataset.reload_streams(force_load=True)\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\schema\\\\__init__.py:223, in Dataset.populate_streams(self, root, autoload)\n",
    "    221     root = ComplexPath(root)\n",
    "    222 root = ensure_complexpath(root)\n",
    "--> 223 self.streams = self.schema(root=root, parent_dataset=self, autoload=autoload)\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\src\\\\ingestion\\\\missing_sync.py:43, in build_schema(root, parent_dataset, autoload)\n",
    "     41 # Pupil streams\n",
    "     42 streams.PupilLabs.DecodedFrames =       HarpStream(209, device=\\\"PupilLabs\\\", streamlabel=\\\"Pupil_RawFrames\\\", **kwargs)\n",
    "---> 43 streams.PupilLabs.RawFrames =           PupilWorldCameraStream(210, device=\\\"PupilLabs\\\", streamlabel=\\\"Pupil_RawFrames\\\", **kwargs)\n",
    "     44 streams.PupilLabs.Counter.IMU =         HarpStream(211, device=\\\"PupilLabs\\\", streamlabel=\\\"Counter_IMU\\\", **kwargs)\n",
    "     45 streams.PupilLabs.PupilGaze =           PupilGazeStream(212, device=\\\"PupilLabs\\\", streamlabel=\\\"Pupil_Gaze\\\", **kwargs)\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\\pluma\\\\stream\\\\zeromq.py:73, in PupilWorldCameraStream.__init__(self, eventcode, **kw)\n",
    "     63 def __init__(self, eventcode: int, **kw):\n",
    "     64     super(PupilWorldCameraStream, self).__init__(\n",
    "     65         eventcode,\n",
    "     66         streamtype=StreamType.PUPIL,\n",
    "     67         filenames=[\n",
    "     68             \\\"PupilLabs/WorldCamera_Frame0.bin\\\",\n",
    "     69             \\\"PupilLabs/WorldCamera_Frame1.bin\\\",\n",
    "     70             \\\"PupilLabs/WorldCamera_Frame2.bin\\\",\n",
    "     71         ],\n",
    "     72         dtypes=[\n",
    "---> 73             [(\\\"SensorId\\\", np.string_, 36)],\n",
    "     74             [\n",
    "     75                 (\\\"Format\\\", np.uint32),\n",
    "     76                 (\\\"Width\\\", np.uint32),\n",
    "     77                 (\\\"Height\\\", np.uint32),\n",
    "     78                 (\\\"Sequence\\\", np.uint32),\n",
    "     79                 (\\\"Timestamp\\\", np.uint64),\n",
    "     80                 (\\\"DataBytes\\\", np.uint32),\n",
    "     81                 (\\\"Reserved\\\", np.uint32),\n",
    "     82             ],\n",
    "     83             None,\n",
    "     84         ],\n",
    "     85         **kw,\n",
    "     86     )\n",
    "\n",
    "File c:\\\\Users\\\\joaop\\\\git\\\\emotional-cities\\\n",
    "otebooks\\\\.conda\\\\Lib\\\\site-packages\\\n",
    "umpy\\\\__init__.py:400, in __getattr__(attr)\n",
    "    397     raise AttributeError(__former_attrs__[attr], name=None)\n",
    "    399 if attr in __expired_attributes__:\n",
    "--> 400     raise AttributeError(\n",
    "    401         f\\\"`np.{attr}` was removed in the NumPy 2.0 release. \\\"\n",
    "    402         f\\\"{__expired_attributes__[attr]}\\\",\n",
    "    403         name=None\n",
    "    404     )\n",
    "    406 if attr == \\\"chararray\\\":\n",
    "    407     warnings.warn(\n",
    "    408         \\\"`np.chararray` is deprecated and will be removed from \\\"\n",
    "    409         \\\"the main namespace in the future. Use an array with a string \\\"\n",
    "    410         \\\"or bytes dtype instead.\\\", DeprecationWarning, stacklevel=2)\n",
    "\n",
    "AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.\"\n",
    "}\n",
    "```\n",
    "Solution:\n",
    "```python\n",
    "name: .conda\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - numpy=1.24.4\n",
    "  - jupyter\n",
    "  - matplotlib\n",
    "  - geopandas\n",
    "  - ipympl\n",
    "  - pip\n",
    "  - pip:\n",
    "    - ipyfilechooser\n",
    "    - opencv-python\n",
    "    - git+https://github.com/emotional-cities/pluma-analysis.git@46cffe14b514d64a83483dbd07c93872ddc9577e\n",
    "```\n",
    "Confirmed version with `pip install \"numpy<2.0\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
